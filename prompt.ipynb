{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset, \n",
    "    load_metric, \n",
    "    load_from_disk, \n",
    "    concatenate_datasets\n",
    ")\n",
    "# from transformers import (\n",
    "#     AutoConfig,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModel,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "# )\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference & Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_structure_data(server):\n",
    "    \"\"\"Get labels for the structured data.\"\"\"\n",
    "    dev_to_path = {\n",
    "        k: [os.path.join(AIT_DATA_ROOT, 'data', v, server), os.path.join(AIT_DATA_ROOT, 'labels', v, server)]\n",
    "        for k,v in AIT_NAME_DICT.items()\n",
    "    }\n",
    "    label_df = {'log': [], 'label': [], 'device': []}\n",
    "    for dev, path_pair in dev_to_path.items():\n",
    "        label_df['log'].extend(pd.read_csv(path_pair[0], names=['log'])['log'].to_list())\n",
    "        label_col = pd.read_csv(path_pair[1], header=None)\n",
    "        label_df['label'].extend([(str(l1), str(l2)) for l1, l2 in zip(label_col[0], label_col[1])])\n",
    "        label_df['device'].extend([dev for _ in range(len(label_col))])\n",
    "\n",
    "    label_df = pd.DataFrame(label_df)\n",
    "    temp_dir = 'dataset/slogert/' + server + '/2-logpai/'\n",
    "    struct_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('structured.csv')]\n",
    "    struct_df = pd.read_csv(struct_files[0], delimiter=\",\")\n",
    "    if len(struct_files) > 1:\n",
    "        for file_path in struct_files[1:]:\n",
    "            struct_df = struct_df.append(pd.read_csv(file_path, delimiter=\",\"), ignore_index=True)\n",
    "    struct_df['Label'] = label_df['label']\n",
    "    print('label set:', set(tuple(x) for x in struct_df['Label']))\n",
    "    struct_dataset = Dataset.from_dict(struct_df)\n",
    "    print('structure', struct_dataset)\n",
    "    print('example[0]:', struct_dataset[0])\n",
    "    return struct_df, struct_dataset\n",
    "\n",
    "# Predict and store results on templates\n",
    "def gen_entities_from_patterns(struct_df, struct_dataset):\n",
    "    preds_pattern = {}\n",
    "    for eventID, insIDs in struct_df.groupby(['EventId']).groups.items():\n",
    "        instance = struct_dataset[int(insIDs[0])] # pick the first instance of each group\n",
    "        print(instance['Content'])\n",
    "        preds = prediction(instance['Content'])  # token classification\n",
    "        print('Pred:', preds)\n",
    "        entities = list(get_entities_bio(preds)) # merge tokens within the same entity\n",
    "        entities.sort(key=lambda x: x[1])\n",
    "        # print('Extracted entities:', entities)\n",
    "        input_tokens = list(filter(None, re.split(TOKENIZE_PATTERN, instance['Content'])))  \n",
    "        ent_list = [(tag, ' '.join(input_tokens[start:end+1])) for (tag, start, end) in entities]\n",
    "        print('Extracted entities:', ent_list)\n",
    "        preds_pattern[eventID] = entities\n",
    "\n",
    "    preds = []\n",
    "    for i, instance in enumerate(struct_dataset):\n",
    "        ent_ids = preds_pattern[instance['EventId']]\n",
    "        log = instance['Content']\n",
    "        input_tokens = list(filter(None, re.split(TOKENIZE_PATTERN, log))) \n",
    "        ent_list = [(tag, ' '.join(input_tokens[start:end+1])) for (tag, start, end) in ent_ids]\n",
    "        preds.append(ent_list)\n",
    "    \n",
    "    struct_dataset = struct_dataset.add_column('Preds', preds)\n",
    "    return preds_pattern, struct_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auth.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = 'dataset/processed'\n",
    "server0 = 'auth.log' # Handle auth.log \n",
    "out_file0 = 'auth-log-dataset-processed.json'\n",
    "\n",
    "if not os.path.isdir(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR)\n",
    "\n",
    "out_path0 = os.path.join(PROCESSED_DIR, out_file0)\n",
    "if os.path.exists(out_path0):\n",
    "    struct_dataset0 = load_from_disk(out_path0)\n",
    "else:\n",
    "    struct_df0, struct_dataset0 = gen_structure_data(server0)\n",
    "    preds_pattern0, struct_dataset0 = gen_entities_from_patterns(struct_df0, struct_dataset0)\n",
    "    struct_dataset0.save_to_disk(out_path0)\n",
    "\n",
    "print(struct_dataset0)\n",
    "print(\"example[0]:\", struct_dataset0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daemon.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server1 = 'daemon.log' # Handle auth.log\n",
    "out_file1 = 'daemon-log-dataset-processed.json'\n",
    "out_path1 = os.path.join(PROCESSED_DIR, out_file1)\n",
    "\n",
    "if os.path.exists(out_path1):\n",
    "    struct_dataset1 = load_from_disk(out_path1)\n",
    "else:\n",
    "    struct_df1, struct_dataset1 = gen_structure_data(server1)\n",
    "    preds_pattern1, struct_dataset1 = gen_entities_from_patterns(struct_df1, struct_dataset1)\n",
    "    struct_dataset1.save_to_disk(out_path1)\n",
    "\n",
    "print(struct_dataset1)\n",
    "print(\"example[0]:\", struct_dataset1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mail.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server2 = 'mail.log' # Handle auth.log \n",
    "out_file2 = 'mail-log-dataset-processed.json'\n",
    "out_path2 = os.path.join(PROCESSED_DIR, out_file2)\n",
    "\n",
    "if os.path.exists(out_path2):\n",
    "    struct_dataset2 = load_from_disk(out_path2)\n",
    "else:\n",
    "    struct_df2, struct_dataset2 = gen_structure_data(server2)\n",
    "    preds_pattern2, struct_dataset2 = gen_entities_from_patterns(struct_df2, struct_dataset2)\n",
    "    struct_dataset2.save_to_disk(out_path2)\n",
    "\n",
    "print(struct_dataset2)\n",
    "print(\"example[0]:\", struct_dataset2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server3 = 'messages' # Handle auth.log \n",
    "out_file3 = 'messages-dataset-processed.json'\n",
    "out_path3 = os.path.join(PROCESSED_DIR, out_file3)\n",
    "\n",
    "if os.path.exists(out_path3):\n",
    "    struct_dataset3 = load_from_disk(out_path3)\n",
    "else:\n",
    "    struct_df3, struct_dataset3 = gen_structure_data(server3)\n",
    "    preds_pattern3, struct_dataset3 = gen_entities_from_patterns(struct_df3, struct_dataset3)\n",
    "    struct_dataset3.save_to_disk(out_path3)\n",
    "\n",
    "print(struct_dataset3)\n",
    "print(\"example[0]:\", struct_dataset3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syslog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server4 = 'syslog' # Handle auth.log \n",
    "out_file4 = 'syslog-dataset-processed.json'\n",
    "out_path4 = os.path.join(PROCESSED_DIR, out_file4)\n",
    "\n",
    "if os.path.exists(out_path4):\n",
    "    struct_dataset4 = load_from_disk(out_path4)\n",
    "else:\n",
    "    struct_df4, struct_dataset4 = gen_structure_data(server4)\n",
    "    preds_pattern4, struct_dataset4 = gen_entities_from_patterns(struct_df4, struct_dataset4)\n",
    "    struct_dataset4.save_to_disk(out_path4)\n",
    "\n",
    "print(struct_dataset4)\n",
    "print(\"example[0]:\", struct_dataset4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server5 = 'user.log' # Handle user.log \n",
    "out_file5 = 'user-dataset-processed.json'\n",
    "out_path5 = os.path.join(PROCESSED_DIR, out_file5)\n",
    "\n",
    "if os.path.exists(out_path5):\n",
    "    struct_dataset5 = load_from_disk(out_path5)\n",
    "else:\n",
    "    struct_df5, struct_dataset5 = gen_structure_data(server5)\n",
    "    preds_pattern5, struct_dataset5 = gen_entities_from_patterns(struct_df5, struct_dataset5)\n",
    "    struct_dataset5.save_to_disk(out_path5)\n",
    "\n",
    "print(struct_dataset5)\n",
    "print(\"example[0]:\", struct_dataset5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Grouped Datasets (by time interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Day-Month-Time into numerical values\n",
    "def time2value(examples):\n",
    "    stamplist, datelist = [], []\n",
    "    for i, hms_time in enumerate(examples['Time']):\n",
    "        mon = examples['Month'][i]\n",
    "        day = examples['Date'][i]\n",
    "        mon_number = datetime.strptime(mon, '%b').month\n",
    "        date_string = f'2020-{mon_number}-{day} {hms_time}' # standard datetime string\n",
    "        datelist.append(date_string)\n",
    "        dt = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").timetuple()\n",
    "        allsecs = time.mktime(dt)\n",
    "        stamplist.append(allsecs) # timestamp (numerical) values\n",
    "\n",
    "    examples['Timestamp'] = stamplist\n",
    "    examples['Datetime'] = datelist\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Merge all hosts and sort logs by Timestamp\n",
    "PROCESSED_DIR = 'dataset/AIT-LDS-v1_1/processed'\n",
    "out_file = 'whole-dataset-processed.json'\n",
    "out_path = os.path.join(PROCESSED_DIR, out_file)\n",
    "if os.path.exists(out_path):\n",
    "    whole_dataset = load_from_disk(out_path)\n",
    "else:\n",
    "    whole_dataset = concatenate_datasets(\n",
    "        [struct_dataset0, struct_dataset1, struct_dataset2, struct_dataset3, struct_dataset4, struct_dataset5]\n",
    "    )\n",
    "    whole_dataset = whole_dataset.map(time2value, batched=True, load_from_cache_file=None)\n",
    "    whole_dataset = whole_dataset.remove_columns(['Month', 'Date', 'Time'])\n",
    "    whole_dataset = whole_dataset.sort('Timestamp') # sort by timestamp\n",
    "    whole_dataset.save_to_disk(out_path)\n",
    "\n",
    "print('whole', whole_dataset)\n",
    "# print(\"example[0]:\", whole_dataset[0])\n",
    "whole_df = whole_dataset.to_pandas()\n",
    "whole_df.head()\n",
    "\n",
    "# Get statistics\n",
    "df_labels = whole_df.Label.apply(lambda x: 0 if set(x) == set('0') else 1)\n",
    "print('# anomalies', sum(df_labels))\n",
    "print('# events', len(whole_df.EventId.unique()))\n",
    "print('average length', np.mean([len(row.Content) for idx, row in whole_df.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset.to_json('dataset/AIT/AIT.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Graphs (by time interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitbyinterval(df, interval):\n",
    "    new_df = df.copy(deep=True)\n",
    "    new_df['Datetime'] = pd.to_datetime(new_df['Datetime'], errors='coerce')\n",
    "    period = new_df.groupby(pd.Grouper(key='Datetime', freq=interval)).ngroup()\n",
    "    new_df['Period'] = np.char.add('period_', (pd.factorize(period)[0]).astype(str))\n",
    "    return new_df\n",
    "\n",
    "def gen_period_graph(dataframe, root_dir):\n",
    "    num_unique_intervals = len(dataframe.Period.unique())\n",
    "    graph_stats = []\n",
    "    stats_file = os.path.join(root_dir, 'graph_stats.json')\n",
    "    invalid_entities = set([('ip', '127.0.0.1'), ('pid', '0')]) # filtering invalid entities\n",
    "    # num_unique_intervals\n",
    "    for period_id in tqdm(range(num_unique_intervals)):\n",
    "        df_sub = dataframe.loc[dataframe.Period == f'period_{period_id}'] # get subdf for current group\n",
    "        num_logs = len(df_sub) # count number of logs in current group\n",
    "        num_anomaly = len(df_sub.loc[df_sub.Label.apply(lambda x: '0' not in x)]) # count anomaly logs in this group\n",
    "        anomaly_rate = num_anomaly/num_logs if num_logs else 0 # calculate anomaly rate\n",
    "        # Store graph\n",
    "        out_normal_file = os.path.join(root_dir, f'period_{period_id}.txt')\n",
    "        gen_file = os.path.join(root_dir, f'period_{period_id}.html')\n",
    "        # Generate entity-tag pairs\n",
    "        with open(out_normal_file, 'w') as f:\n",
    "            # for entities in df_sub['Preds']: # for each log \n",
    "            #     for i in range(len(entities)-1):\n",
    "            #         for j in range(i+1, len(entities)):\n",
    "            #             # tag0, entity0, tag1, entity1\n",
    "            #             f.write(entities[i][0]+'\\t'+entities[i][1]+'\\t'+entities[j][0]+'\\t'+entities[j][1])\n",
    "            #             f.write('\\n')\n",
    "            for idx in range(num_logs):\n",
    "                instance = df_sub.iloc[idx]\n",
    "                # Connect eventID to Component\n",
    "                component = re.split('([\\[\\]])', instance.Component)[0] # remove [XXXX] part\n",
    "                f.write('event' + '\\t' + instance.EventId + '\\t' + 'component' + '\\t' + component)\n",
    "                f.write('\\n')\n",
    "                # Connect Component to Device\n",
    "                f.write('component' + '\\t' + component + '\\t' + 'device' + '\\t' + instance.Device)\n",
    "                f.write('\\n')\n",
    "                # Connect eventID to each entity\n",
    "                for entity in instance.Preds:\n",
    "                    if tuple(entity) not in invalid_entities: # valid entity pair\n",
    "                        f.write('event' + '\\t' + instance.EventId + '\\t' + entity[0] + '\\t' + entity[1])\n",
    "                        f.write('\\n')\n",
    "                \n",
    "        f.close()\n",
    "        # Generate graph visualization file\n",
    "        visualize(out_normal_file, gen_file)\n",
    "        ent_count, edge_count = read_hyper(out_normal_file)\n",
    "        num_edges = len(edge_count)\n",
    "        num_nodes = len(ent_count)\n",
    "        avg_degree = num_edges/num_nodes if num_nodes else 0\n",
    "        avg_degree = math.ceil(avg_degree*10000)/10000\n",
    "        stats = {'graph_ID': period_id, '#nodes': num_nodes, '#edges': num_edges, 'degree': avg_degree, '#log': num_logs, '#anomaly': num_anomaly, 'anomaly_rate': anomaly_rate}\n",
    "        graph_stats.append(stats)\n",
    "\n",
    "    with open(stats_file, 'w') as f:\n",
    "        for stats in graph_stats: # for each log \n",
    "            f.write(json.dumps(stats))\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by time interval\n",
    "# interval = '10min'\n",
    "# interval = '5min'\n",
    "interval = '2min'\n",
    "grouped_df = splitbyinterval(whole_df, interval)\n",
    "demo_dir = os.path.join('dataset/new_graph', interval)\n",
    "\n",
    "if not os.path.exists(demo_dir):\n",
    "    os.makedirs(demo_dir)\n",
    "\n",
    "# Generate subgraphs based on time interval\n",
    "# gen_period_graph(grouped_df, demo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df_path = '/nfs/intern_data/yufli/dataset/AIT_0.5min_df.csv'\n",
    "data_df = pd.read_csv(common_df_path, engine='c', na_filter=False, memory_map=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDFS dataset demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loglizer.models import *\n",
    "from loglizer import dataloader, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_log = 'dataset/HDFS-demo/HDFS_100k.log_structured.csv' # The structured log file\n",
    "label_file = 'dataset/HDFS-demo/anomaly_label.csv' # The anomaly label file\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_HDFS(\n",
    "    struct_log,\n",
    "    label_file=label_file,\n",
    "    window='session', \n",
    "    train_ratio=0.5,\n",
    "    split_type='uniform'\n",
    ")\n",
    "\n",
    "feature_extractor = preprocessing.FeatureExtractor()\n",
    "x_train = feature_extractor.fit_transform(x_train, term_weighting='tf-idf')\n",
    "x_test = feature_extractor.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()\n",
    "model.fit(x_train, y_train)\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "\n",
    "print('Test validation:')\n",
    "precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on HDFS benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'dataset/HDFS-demo/HDFS_100k.log_structured.csv' # the structured log file\n",
    "label_file = 'dataset/HDFS-demo/anomaly_label.csv' # the anomaly label file\n",
    "struct_df = pd.read_csv(log_file, na_filter=False, memory_map=True)\n",
    "struct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import add_preds_to_df\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "bart_model = AutoModel.from_pretrained(f\"results/BART_seq2seq/10-shot-0\")\n",
    "add_preds_to_df(struct_df, 'regex', bart_model, tokenizer, 0)\n",
    "\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get blockId and corresponding logs\n",
    "print(\"Getting BlockIDs and Logs!!! Total number of logs: {}\".format(struct_df.shape[0]))\n",
    "data_dict = OrderedDict()\n",
    "for idx, row in tqdm(struct_df.iterrows()):\n",
    "    blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "    blkId_set = set(blkId_list)\n",
    "    for blk_Id in blkId_set:\n",
    "        if not blk_Id in data_dict:\n",
    "            data_dict[blk_Id] = defaultdict(list)\n",
    "            data_dict[blk_Id]['BlockId'] = blk_Id\n",
    "        for col in struct_df.columns:\n",
    "            data_dict[blk_Id][col].append(row[col])\n",
    "\n",
    "data_df = pd.DataFrame(data_dict.values())\n",
    "\n",
    "# Add labels to each block \n",
    "label_data = pd.read_csv(label_file, engine='c', na_filter=False, memory_map=True)\n",
    "label_data = label_data.set_index('BlockId')\n",
    "label_dict = label_data['Label'].to_dict()\n",
    "data_df['Label'] = data_df['BlockId'].apply(lambda x: 1 if label_dict[x] == 'Anomaly' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "print(\"Splitting graph datasets!!!\")\n",
    "num_total = data_df.shape[0]\n",
    "normal_samples = data_df[data_df.Label == 0]\n",
    "anomaly_samples = data_df[data_df.Label == 1]\n",
    "num_normal = normal_samples.shape[0]\n",
    "num_anomaly = anomaly_samples.shape[0]\n",
    "anomaly_rate = num_anomaly/num_total if num_total else 0\n",
    "\n",
    "train_df, test_normal_df = train_test_split(normal_samples, test_size=0.2, random_state=seed)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=seed)\n",
    "test_df = pd.concat([anomaly_samples, test_normal_df], ignore_index=True)\n",
    "test_anomaly_rate = num_anomaly/test_df.shape[0] if test_df.shape[0] else 0\n",
    "\n",
    "print(\"Total number of graphs: {}, normal graphs: {}, anomaly graphs: {}, anomaly ratio: {:.4f}\".format(\n",
    "    num_total, num_normal, num_anomaly, anomaly_rate))\n",
    "print(\"Train data size: {}, validation data size: {}, test data size: {}, test anomaly ratio: {:.4f}\".format(\n",
    "    train_df.shape[0], val_df.shape[0], test_df.shape[0], test_anomaly_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch geometric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset import HDFSDataset\n",
    "\n",
    "# Test graph dataset\n",
    "root = 'dataset/HDFS/regex'\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "tag2id['date'] = len(tag2id) # for hdfs dataset\n",
    "# df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    plm=plm,\n",
    "    ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    ")\n",
    "hdfs_data = HDFSDataset(root, hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics (using huggingface dataset)\n",
    "hdfs_stats_data = hdfs_data.graph_stats\n",
    "print(hdfs_stats_data)\n",
    "# Retrieve normal and anomaly indices\n",
    "anomaly_ids = [i for i, y in enumerate(hdfs_stats_data['label']) if y == 1]\n",
    "normal_ids = [i for i, y in enumerate(hdfs_stats_data['label']) if y == 0]\n",
    "print(\"#anomly {}, #normal {}\".format(len(anomaly_ids), len(normal_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "k = 10\n",
    "# randomly chose k normal and anomaly graphs\n",
    "a_sub_ids = random.sample(anomaly_ids, k=k)\n",
    "for idx in a_sub_ids:\n",
    "    hdfs_data._visualize(idx)\n",
    "n_sub_ids = random.sample(normal_ids, k=k)\n",
    "for idx in n_sub_ids:\n",
    "    hdfs_data._visualize(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGL dataset few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'dataset/BGL/BGL.log_structured.csv' # the structured log file\n",
    "bgl_df = pd.read_csv(log_file, na_filter=False, memory_map=True)\n",
    "bgl_df['Tag'] = bgl_df['Label'].apply(lambda x: 0 if x == '-' else 1)\n",
    "bgl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import splitbyinterval\n",
    "\n",
    "bgl_demo_df = bgl_df.sample(frac=0.1) # 471349\n",
    "grouped_df = splitbyinterval(bgl_demo_df, interval='2min')\n",
    "data_dict = OrderedDict()\n",
    "for idx, row in tqdm(grouped_df.iterrows()):\n",
    "    group_id = row['Period']\n",
    "    if group_id not in data_dict:\n",
    "        data_dict[group_id] = defaultdict(list)\n",
    "\n",
    "    for col in grouped_df.columns:\n",
    "            data_dict[group_id][col].append(row[col])\n",
    "    data_dict[group_id]\n",
    "\n",
    "bgl_data_df = pd.DataFrame(data_dict.values())\n",
    "# Add labels to each group\n",
    "bgl_data_df['EventLabels'] = bgl_data_df['Label'].apply(lambda x: [0 if item=='-' else 1 for item in x])\n",
    "bgl_data_df['Label'] = bgl_data_df['Label'].apply(lambda x: 0 if set(x) == set('-') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BGL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset import BGLDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Test graph dataset\n",
    "root = 'dataset/BGL/seq2seq-node-0.5min-template-bertembed/'\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "# df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    # plm=plm,\n",
    "    # ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    ")\n",
    "bgl_data = BGLDataset(root, hparams=hparams)\n",
    "bgl_loader = DataLoader(bgl_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics (using huggingface dataset)\n",
    "bgl_stats_data = bgl_data.graph_stats\n",
    "print(bgl_stats_data) # |G| = 36169\n",
    "print(bgl_stats_data[0])\n",
    "# # Retrieve normal and anomaly indices\n",
    "# anomaly_ids = [i for i, y in enumerate(bgl_stats_data['label']) if y == 1]\n",
    "# normal_ids = [i for i, y in enumerate(bgl_stats_data['label']) if y == 0]\n",
    "# print(\"#anomly {}, #normal {}\".format(len(anomaly_ids), len(normal_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using template as node attribute input\n",
    "model_path = 'bert-large-uncased' # 'bert-large-uncased', 'facebook/bart-large', 'results/BART_seq2seq/bgl/10-shot-0-10negrate/'\n",
    "plm = AutoModel.from_pretrained(model_path)\n",
    "ptk = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "strategy=0\n",
    "batch_size=46\n",
    "\n",
    "graph_dict = bgl_stats_data[11]\n",
    "ent_tag_dict = {x[1]:x[0] for x in graph_dict['nodes']} # {entity: tag}\n",
    "graph_prompts = []\n",
    "\n",
    "for ent, tag in ent_tag_dict.items():\n",
    "    if tag == 'event':\n",
    "        prompt = ent + ' is an event ID .'\n",
    "    elif tag == 'component':\n",
    "        prompt = ent + ' is a log message component .'\n",
    "    else:\n",
    "        prompt = ent + LABEL2TEMPLATE[tag][strategy]\n",
    "    # print(prompt)\n",
    "    graph_prompts.append(prompt)\n",
    "\n",
    "# print(len(graph_prompts))\n",
    "# Batch handling\n",
    "if len(graph_prompts) > batch_size:\n",
    "    feature = []\n",
    "    num_batch = math.ceil(len(graph_prompts)/batch_size)\n",
    "    for i in range(num_batch):\n",
    "        batch_prompts = graph_prompts[i*batch_size: min(len(graph_prompts), (i+1)*batch_size)]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized_inputs = ptk(\n",
    "            batch_prompts, \n",
    "            max_length=1024,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Pad dynamically \n",
    "        batch = ptk.pad(\n",
    "            tokenized_inputs,\n",
    "            padding=True,\n",
    "            max_length=1024,\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors=\"pt\",\n",
    "        ) # ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "        # Encode\n",
    "        encode_outputs = plm(**batch) # ['last_hidden_state', 'pooler_output']\n",
    "        feature.append(encode_outputs.pooler_output.detach().cpu()) # B X H\n",
    "\n",
    "    feature = torch.cat(feature, dim=0)\n",
    "    print(feature.shape)\n",
    "\n",
    "else:\n",
    "    # Tokenize\n",
    "    tokenized_inputs = ptk(\n",
    "        graph_prompts, \n",
    "        max_length=1024,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Pad dynamically \n",
    "    batch = ptk.pad(\n",
    "        tokenized_inputs,\n",
    "        padding=True,\n",
    "        max_length=1024,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_tensors=\"pt\",\n",
    "    ) # ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "    # Encode\n",
    "    encode_outputs = plm(**batch) # ['last_hidden_state', 'pooler_output']\n",
    "    feature = encode_outputs.pooler_output.detach().cpu() # B X H\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "k = 10\n",
    "# randomly chose k normal and anomaly graphs\n",
    "a_sub_ids = random.sample(range(len(bgl_data)), k=k)\n",
    "for idx in a_sub_ids:\n",
    "    bgl_data._visualize(idx)\n",
    "# n_sub_ids = random.sample(normal_ids, k=k)\n",
    "# for idx in n_sub_ids:\n",
    "#     bgl_data._visualize(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_model import GCNGraphEmbedding, GCNNodeEmbedding\n",
    "\n",
    "m = GCNGraphEmbedding(0.5, 1041, 128, 3)\n",
    "data0 = bgl_data[0]\n",
    "for i, batch in enumerate(bgl_loader):\n",
    "    if i == 0:\n",
    "        batch0 = batch\n",
    "        break\n",
    "outputs = m(x=data0['x'], \n",
    "    edge_index=data0['edge_index'], \n",
    "    batch = torch.LongTensor([0]*data0['x'].shape[0])\n",
    ")\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGL Node Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "\n",
    "root = 'dataset/BGL/regex-node-2min'\n",
    "# Test graph dataset\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained('t5-large')\n",
    "# seq2seq = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'ae-gcnae'}\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    plm=plm,\n",
    "    ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "bgl_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "bgl_node_loader = DataLoader(bgl_node_data, batch_size=64, shuffle=False)\n",
    "node_model = AENodeConv(hparams)\n",
    "node_batch = next(iter(bgl_node_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics (using huggingface dataset)\n",
    "bgl_node_stats_data = bgl_node_data.graph_stats\n",
    "print(bgl_node_stats_data)\n",
    "# Retrieve normal and anomaly indices\n",
    "anomaly_ids = [i for i, y in enumerate(bgl_node_stats_data['label']) if sum(y) > 0]\n",
    "normal_ids = [i for i, y in enumerate(bgl_node_stats_data['label']) if sum(y) == 0]\n",
    "print(\"#anomly {}, #normal {}\".format(len(anomaly_ids), len(normal_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "k = 10\n",
    "# randomly chose k normal and anomaly graphs\n",
    "a_sub_ids = random.sample(anomaly_ids, k=k)\n",
    "for idx in a_sub_ids:\n",
    "    bgl_node_data._visualize(idx)\n",
    "n_sub_ids = random.sample(normal_ids, k=k)\n",
    "for idx in n_sub_ids:\n",
    "    bgl_node_data._visualize(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly case study (BGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'dataset/BGL/BGL.log_structured.csv' # the structured log file\n",
    "bgl_df = pd.read_csv(log_file, na_filter=False, memory_map=True)\n",
    "bgl_df['Tag'] = bgl_df['Label'].apply(lambda x: 0 if x == '-' else 1)\n",
    "bgl_df['Datetime'] = bgl_df['Timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "print(bgl_df.shape)\n",
    "bgl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import splitbyinterval, get_train_test_data\n",
    "\n",
    "# sub_bgl_df = bgl_df.loc[bgl_df.LineId <= 100000]\n",
    "interval = '0.5min'\n",
    "grouped_df = splitbyinterval(bgl_df, interval)\n",
    "data_dict = OrderedDict()\n",
    "for idx, row in tqdm(grouped_df.iterrows()):\n",
    "    group_id = row['Period']\n",
    "    if group_id not in data_dict:\n",
    "        data_dict[group_id] = defaultdict(list)\n",
    "\n",
    "    for col in grouped_df.columns:\n",
    "        data_dict[group_id][col].append(row[col])\n",
    "    data_dict[group_id]\n",
    "\n",
    "data_df = pd.DataFrame(data_dict.values())\n",
    "# Add labels to each group\n",
    "data_df['EventLabels'] = data_df['Label'].apply(lambda x: [0 if item=='-' else 1 for item in x])\n",
    "data_df['Label'] = data_df['Label'].apply(lambda x: 0 if set(x) == set('-') else 1)\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_event_df = bgl_df.groupby('EventId').head(1)[['Content', 'EventTemplate', 'ParameterList']]\n",
    "bgl_event_df['ner_tags'] = [[] for _ in range(len(bgl_event_df))]\n",
    "print(bgl_event_df.shape)\n",
    "bgl_event_df.to_csv('dataset/BGL/BGL.log_structured_event_1.csv')\n",
    "train_event_df = bgl_event_df.sample(n=500, random_state=seed)\n",
    "train_event_df.to_csv('dataset/BGL/BGL.log_structured_event_train.csv', index=False)\n",
    "remain_event_df = bgl_event_df[~bgl_event_df.index.isin(train_event_df.index)]\n",
    "test_event_df = remain_event_df.sample(frac=0.8, random_state=seed)\n",
    "val_event_df = remain_event_df[~remain_event_df.index.isin(test_event_df.index)]\n",
    "val_event_df.to_csv('dataset/BGL/BGL.log_structured_event_val.csv', index=False)\n",
    "test_event_df.to_csv('dataset/BGL/BGL.log_structured_event_test.csv', index=False)\n",
    "print(bgl_event_df.shape, train_event_df.shape, val_event_df.shape, test_event_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "t_file = 'dataset/BGL/BGL.log_structured_event_train.csv'\n",
    "v_file = 'dataset/BGL/BGL.log_structured_event_val.csv'\n",
    "\n",
    "def process_csv(file):\n",
    "    t_df = pd.read_csv(file)\n",
    "    # Parse string of list\n",
    "    t_df.ParameterList = t_df.ParameterList.apply(literal_eval)\n",
    "    t_df.ner_tags = t_df.ner_tags.apply(literal_eval)\n",
    "    # Get rid of unnamed columns\n",
    "    if 'Unnamed: 0' in t_df.columns:\n",
    "        t_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    # Test correspondence\n",
    "    for i, row in t_df.iterrows():\n",
    "        if len(row.ParameterList) != len(row.ner_tags):\n",
    "            raise ValueError('number of named entities does not match tags!')\n",
    "    return t_df\n",
    "\n",
    "df1 = process_csv(t_file)\n",
    "df2 = process_csv(v_file)\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = process_csv(t_file)\n",
    "df.rename(columns={\n",
    "    \"Content\": \"logex:example\", \n",
    "    \"EventTemplate\": \"logex:pattern\", \n",
    "    \"ParameterList\": \"logex:hasParameterList\", \n",
    "    \"ner_tags\": \"logex:hasNERtag\",\n",
    "}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ENTITY_COLUMN_NAME, TAG_COLUMN_NAME, LOG_COLUMN_NAME\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "data = Dataset.from_pandas(df)\n",
    "\n",
    "# Save dataset\n",
    "out_f = '/nfs/intern_data/yufli/dataset/BGL/dataset.json'\n",
    "data.to_json(out_f)\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset('json', data_files=out_f)['train']\n",
    "\n",
    "# Get tag-entity statistics\n",
    "entity_set = defaultdict(set)\n",
    "entity_count = defaultdict(list)\n",
    "for i, instance in enumerate(data):\n",
    "    for ent, tag in zip(instance[ENTITY_COLUMN_NAME], instance[TAG_COLUMN_NAME]):\n",
    "            entity_set[tag].add(ent)\n",
    "            entity_count[tag].append(i)\n",
    "\n",
    "entity_occ = sum(len(ids) for ids in entity_count.values())\n",
    "print(\"Total #entities: {}, average #entities per log: {:.3f}\".format(entity_occ, entity_occ/len(data)))\n",
    "print(\"Entity distribution ({}): {}\".format(len(entity_count), {k:len(v) for k,v in entity_count.items()}))\n",
    "print('log: \"%s\",'%data[LOG_COLUMN_NAME][0], \n",
    "        'entities: %s,'%data[ENTITY_COLUMN_NAME][0], \n",
    "        'tags: %s.'%data[TAG_COLUMN_NAME][0])\n",
    "for tag, entities in entity_set.items():\n",
    "    print(\"\\t{} ({}): {}\".format(tag, len(entities), entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train(10-shot & 5-shot)/val/test data for NER training\n",
    "n_shots = 10\n",
    "n_shot_ids = []\n",
    "ten_shot_ids = []\n",
    "random.seed(seed)\n",
    "for tag in entity_count:\n",
    "    tag_ids = random.choices(entity_count[tag], k=10) \n",
    "    ten_shot_ids.extend(tag_ids) # 10-shot\n",
    "    n_shot_ids.extend(tag_ids[:n_shots])\n",
    "\n",
    "n_shot_data = data.select(n_shot_ids).shuffle(seed=seed) # n-shot\n",
    "remain_ids = list(set(range(len(data))) - set(ten_shot_ids)) \n",
    "val_ids = random.sample(remain_ids, int(len(remain_ids)*0.3))\n",
    "val_data = data.select(val_ids)\n",
    "test_ids = list(set(remain_ids) - set(val_ids))\n",
    "test_data = data.select(test_ids)\n",
    "print(n_shot_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_anomaly_rate_dict = defaultdict(float)\n",
    "\n",
    "for eventid, ids in bgl_df.groupby('EventId').groups.items():\n",
    "    subgroup = bgl_df.loc[ids]\n",
    "    event_anomaly_rate_dict[eventid] = sum(subgroup.Tag)/len(ids)\n",
    "\n",
    "event_anomaly_rate = sorted(event_anomaly_rate_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(len(event_anomaly_rate))\n",
    "print(event_anomaly_rate[:57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "all_labels = np.array([0 if sum(x) == 0 else 1 for x in bgl_node_data.graph_stats['label']])\n",
    "anomaly_size = sum(all_labels)\n",
    "normal_size = len(all_labels) - anomaly_size\n",
    "\n",
    "n_train = floor(normal_size*0.8) \n",
    "val_size = ceil(n_train*0.2)\n",
    "train_size = floor(n_train*0.8)\n",
    "test_size = len(bgl_node_data) - train_size - val_size\n",
    "test_anomaly_rate = anomaly_size/test_size\n",
    "\n",
    "train_graph_data = bgl_node_data[:train_size]\n",
    "val_graph_data = bgl_node_data[train_size:train_size + val_size]\n",
    "test_graph_data = bgl_node_data[train_size + val_size:]\n",
    "\n",
    "print(len(train_graph_data), len(val_graph_data), len(test_graph_data))\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_graph_data, \n",
    "    batch_size=256, \n",
    "    shuffle=False, \n",
    ")\n",
    "\n",
    "event_id = EMBED_SIZE + tag2id['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = []\n",
    "\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    is_event_nodes = batch['x'][:, event_id] == 1\n",
    "    labels = batch['y'][is_event_nodes]\n",
    "    test_gt.append(labels)\n",
    "\n",
    "test_gt = torch.cat(test_gt, dim=0).numpy() # N (only for event predictions)\n",
    "print(len(test_gt), sum(test_gt), sum(test_gt)/len(test_gt))\n",
    "test_node_stats = bgl_node_data.graph_stats.select(range(train_size + val_size, len(bgl_node_data)))\n",
    "test_node_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.read_csv('results/bgl/regex-node/GCN-mlp-2min-new/predictions.csv', na_filter=False, memory_map=True)\n",
    "test_pred.drop(test_pred.columns[test_pred.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "\n",
    "event_count = []\n",
    "idx = 0\n",
    "\n",
    "for graph_id, graph_dict in tqdm(enumerate(test_node_stats)):\n",
    "    ent_tag_dict = {x[1]:x[0] for x in graph_dict['nodes']} # {entity: tag}\n",
    "    for ent, tag in ent_tag_dict.items():\n",
    "        if tag == 'event':\n",
    "            value = {'eventId': ent, 'graphId': train_size + val_size + graph_id}\n",
    "            row = test_pred.iloc[idx]\n",
    "            for col in test_pred.columns:\n",
    "                value[col] = row[col]\n",
    "            event_count.append(value)\n",
    "            idx += 1\n",
    "\n",
    "event_pred_df = pd.DataFrame(event_count)\n",
    "event_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stats = {}\n",
    "TP_graph_set, FN_graph_set, FP_graph_set, TN_graph_set = set(), set(), set(), set()\n",
    "\n",
    "for idx, row in tqdm(event_pred_df.iterrows()):\n",
    "    y = 'anomaly' if row.GT == 1 else 'normal'\n",
    "    if row.eventId not in event_stats:\n",
    "        event_stats[row.eventId] = {'TP': [], 'FP': [], 'TN': [], 'FN': []}\n",
    "\n",
    "    if row.GT == 1:\n",
    "        if row['top80%'] == 1:\n",
    "            event_stats[row.eventId]['TP'].append(row.graphId)\n",
    "            if row.graphId not in TP_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_TP.html')\n",
    "                TP_graph_set.add(row.graphId)\n",
    "        else:\n",
    "            event_stats[row.eventId]['FN'].append(row.graphId)\n",
    "            if row.graphId not in FN_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_FN.html')\n",
    "                FN_graph_set.add(row.graphId)\n",
    "    else:\n",
    "        if row['top80%'] == 1:\n",
    "            event_stats[row.eventId]['FP'].append(row.graphId)\n",
    "            if row.graphId not in FP_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_TP.html')\n",
    "                FP_graph_set.add(row.graphId)\n",
    "        else:\n",
    "            event_stats[row.eventId]['TN'].append(row.graphId)\n",
    "            if row.graphId not in TN_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_FN.html')\n",
    "                TN_graph_set.add(row.graphId)\n",
    "\n",
    "for event, stats in event_stats.items():\n",
    "    stats['Precision'] = len(stats['TP'])/(len(stats['TP'])+len(stats['FP'])) if len(stats['TP'])+len(stats['FP']) else 0\n",
    "    stats['Recall'] = len(stats['TP'])/(len(stats['TP'])+len(stats['FN'])) if len(stats['TP'])+len(stats['FN']) else 0\n",
    "    stats['F1'] = 2*stats['Precision']*stats['Recall']/(stats['Precision']+stats['Recall']) if stats['Precision']+stats['Recall'] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_events = set([x[0] for x in event_anomaly_rate[:57]])\n",
    "anomaly_event_stats = []\n",
    "for event, stats in event_stats.items():\n",
    "    if event in anomaly_events:\n",
    "        stats['anomaly_rate'] = event_anomaly_rate_dict[event]\n",
    "        stats['eventId'] = event\n",
    "        anomaly_event_stats.append(stats)\n",
    "\n",
    "anomaly_event_df = pd.DataFrame(anomaly_event_stats)\n",
    "cols = anomaly_event_df.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "anomaly_event_df = anomaly_event_df[cols]\n",
    "anomaly_event_df = anomaly_event_df.sort_values(by=['F1', 'Precision'], ascending=False, ignore_index=True)\n",
    "anomaly_event_df.to_csv('results/bgl/regex-node/GCN-mlp-2min-new/event-results-analysis.csv', index=False)\n",
    "anomaly_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(anomaly_event_df.iterrows()):\n",
    "    if len(row.TP) and len(row.FN):\n",
    "        for graphId in row.TP:\n",
    "            visualize(\n",
    "                graph_stats=bgl_node_data.graph_stats, \n",
    "                root=bgl_node_data.root, \n",
    "                out_dir=os.path.join(bgl_node_data.root, 'graph', row.eventId), \n",
    "                idx=graphId, \n",
    "                name=f'graph_{graphId}_TP.html',\n",
    "            )\n",
    "        for graphId in row.FN:\n",
    "            visualize(\n",
    "                graph_stats=bgl_node_data.graph_stats, \n",
    "                root=bgl_node_data.root, \n",
    "                out_dir=os.path.join(bgl_node_data.root, 'graph', row.eventId), \n",
    "                idx=graphId, \n",
    "                name=f'graph_{graphId}_FN.html',\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = EMBED_SIZE + tag2id['event']\n",
    "is_event_nodes = batch['x'][:, event_id] == 1\n",
    "event_x= batch['x'][is_event_nodes]\n",
    "preds = node_model(\n",
    "    x=batch['x'], \n",
    "    edge_index=batch['edge_index'], \n",
    "    batch=batch['batch'],\n",
    ")\n",
    "event_preds = preds[is_event_nodes]\n",
    "event_labels = batch['y'][is_event_nodes]\n",
    "print(event_x.shape, event_preds.shape, event_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch end\n",
    "event2preds = defaultdict(list)\n",
    "event_x_list = event_x.tolist()\n",
    "\n",
    "for x, preds in zip(event_x_list, event_preds):\n",
    "    event2preds[tuple(x)].append(preds)\n",
    "    \n",
    "print(len(event2preds))\n",
    "print([len(v) for k,v in event2preds.items()])\n",
    "print(sum(len(v) == 1 for k,v in event2preds.items()))\n",
    "\n",
    "# Average embeddings for each event\n",
    "event2avg = {k: torch.stack(preds, dim=0).mean(dim=0) for k, preds in event2preds.items()}\n",
    "print(len(event2avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epoch\n",
    "train_dists = defaultdict(list)\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "targets = []\n",
    "for x in event_x_list:\n",
    "    targets.append(event2avg[tuple(x)])\n",
    "\n",
    "targets = torch.stack(targets, dim=0)\n",
    "print(event_preds.size(), targets.size())\n",
    "individual_loss = mse_loss(event_preds, targets).sum(dim=-1) # B\n",
    "\n",
    "for i, x in enumerate(event_x_list):\n",
    "    train_dists[tuple(x)].append(individual_loss[i].detach().item())\n",
    "    \n",
    "print(len(train_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AE Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from argparse import Namespace\n",
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "from SetBart import BartForConditionalGeneration\n",
    "\n",
    "root = '/nfs/intern_data/yufli/dataset/BGL/regex-node-halfmin-template'\n",
    "\n",
    "# Test graph dataset\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# seq2seq = AutoModelForSeq2SeqLM.from_pretrained('t5-large')\n",
    "config = AutoConfig.from_pretrained('facebook/bart-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')\n",
    "seq2seq2 = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'ae-gcnae'}\n",
    "\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    plm=plm,\n",
    "    ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "bgl_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "bgl_node_loader = DataLoader(bgl_node_data, batch_size=8, shuffle=False)\n",
    "node_model = AENodeConv(hparams)\n",
    "\n",
    "# Modify bart VOCAB\n",
    "seq2seq.resize_token_embeddings(bgl_node_data.num_nodes)\n",
    "seq2seq2.resize_token_embeddings(bgl_node_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = bgl_node_data[0]\n",
    "stats0 = bgl_node_data.graph_stats[0]\n",
    "ent_tag_dict0 = {x[1]:x[0] for x in stats0['nodes']} # {entity: tag}\n",
    "print(x0)\n",
    "print(stats0)\n",
    "print(ent_tag_dict0)\n",
    "print(list(ent_tag_dict0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "merged_data = Data(x=x0.x, edge_index=x0.edge_index, y=x0.y, ids=x0.ids, node_strs=list(ent_tag_dict0))\n",
    "print(merged_data)\n",
    "print(merged_data.node_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(bgl_node_loader):\n",
    "    if i == 0:\n",
    "        batch0 = batch\n",
    "    elif i == 1:\n",
    "        batch1 = batch\n",
    "    elif i == 2:\n",
    "        batch2 = batch\n",
    "    elif i == 3:\n",
    "        batch3 = batch\n",
    "    elif i == 4:\n",
    "        batch4 = batch\n",
    "    elif i == 5:\n",
    "        batch5 = batch\n",
    "    elif i == 6:\n",
    "        batch6 = batch\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "out_channels = 128\n",
    "num_layers = 4\n",
    "embed_dim = 768\n",
    "# split the number of layers for the encoder and decoders\n",
    "decoder_layers = int(num_layers / 2)\n",
    "encoder_layers = num_layers - decoder_layers\n",
    "num_neigh = -1\n",
    "alpha = 0.5\n",
    "\n",
    "gcn_encoder = GCN(in_channels=batch0.x.shape[1],\n",
    "            hidden_channels=out_channels,\n",
    "            out_channels=embed_dim,\n",
    "            num_layers=encoder_layers,\n",
    "            dropout=0.3,\n",
    "            act=F.relu)\n",
    "attr_decoder = GCN(in_channels=embed_dim,\n",
    "            hidden_channels=out_channels,\n",
    "            out_channels=batch0.x.shape[1],\n",
    "            num_layers=decoder_layers,\n",
    "            dropout=0.3,\n",
    "            act=F.relu)\n",
    "struct_decoder = GCN(in_channels=embed_dim,\n",
    "            hidden_channels=out_channels,\n",
    "            out_channels=batch0.x.shape[1],\n",
    "            num_layers=decoder_layers - 1,\n",
    "            dropout=0.3,\n",
    "            act=F.relu)\n",
    "\n",
    "def get_half_index(batch, batch_size=8):\n",
    "    half_idx = (batch >= int(batch_size/2)).nonzero()[0].item()\n",
    "    return half_idx\n",
    "\n",
    "def get_preds_from_batch(node_batch, gcn_model):\n",
    "    node_idx = torch.arange(node_batch.x.shape[0])\n",
    "    s = to_dense_adj(node_batch.edge_index)[0] # |V| X |V|\n",
    "    x_ = gcn_model(x=node_batch.x, edge_index=node_batch.edge_index) # |V| X 1024\n",
    "    # Split by half number of graphs in this batch\n",
    "    half_idx = get_half_index(node_batch.batch)\n",
    "    former_batch = batch0.batch[:half_idx]\n",
    "    former_x_ = x_[:half_idx]\n",
    "    former_ids = node_batch.ids[:half_idx]\n",
    "\n",
    "    latter_batch = batch0.batch[half_idx:]\n",
    "    latter_x_ = x_[half_idx:]\n",
    "    latter_ids = node_batch.ids[half_idx:]\n",
    "    \n",
    "    return x_.unsqueeze(0), former_ids.unsqueeze(0), former_x_.unsqueeze(0), \\\n",
    "        latter_ids.unsqueeze(0), latter_x_.unsqueeze(0)\n",
    "\n",
    "def to_sparse_batch(x, mask):\n",
    "    h = x.shape[-1]\n",
    "    if x.dim() == 3:\n",
    "        mask_ = mask.unsqueeze(-1).expand(x.size())\n",
    "        return torch.masked_select(x, mask_).reshape(-1, h) # |V| X 1024\n",
    "    elif x.dim() == 2:\n",
    "        return torch.masked_select(x, mask) # |V|\n",
    "    \n",
    "\n",
    "x0_, x0_former_ids, x0_former_embed, x0_latter_ids, x0_latter_embed = get_preds_from_batch(batch0, gcn_encoder)\n",
    "x1_, x1_former_ids, x1_former_embed, x1_latter_ids, x1_latter_embed = get_preds_from_batch(batch1, gcn_encoder)\n",
    "x2_, x2_former_ids, x2_former_embed, x2_latter_ids, x2_latter_embed = get_preds_from_batch(batch2, gcn_encoder)\n",
    "x3_, x3_former_ids, x3_former_embed, x3_latter_ids, x3_latter_embed = get_preds_from_batch(batch3, gcn_encoder)\n",
    "x4_, x4_former_ids, x4_former_embed, x4_latter_ids, x4_latter_embed = get_preds_from_batch(batch4, gcn_encoder)\n",
    "x5_, x5_former_ids, x5_former_embed, x5_latter_ids, x5_latter_embed = get_preds_from_batch(batch5, gcn_encoder)\n",
    "x6_, x6_former_ids, x6_former_embed, x6_latter_ids, x6_latter_embed = get_preds_from_batch(batch6, gcn_encoder)\n",
    "print([item.shape for item in [x0_, x0_former_ids, x0_former_embed, x0_latter_ids, x0_latter_embed]])\n",
    "print([item.shape for item in [x1_, x1_former_ids, x1_former_embed, x1_latter_ids, x1_latter_embed]])\n",
    "print([item.shape for item in [x2_, x2_former_ids, x2_former_embed, x2_latter_ids, x2_latter_embed]])\n",
    "print([item.shape for item in [x3_, x3_former_ids, x3_former_embed, x3_latter_ids, x3_latter_embed]])\n",
    "print([item.shape for item in [x4_, x4_former_ids, x4_former_embed, x4_latter_ids, x4_latter_embed]])\n",
    "print([item.shape for item in [x5_, x5_former_ids, x5_former_embed, x5_latter_ids, x5_latter_embed]])\n",
    "print([item.shape for item in [x6_, x6_former_ids, x6_former_embed, x6_latter_ids, x6_latter_embed]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "torch.manual_seed(8)\n",
    "\n",
    "# handle node and edge index\n",
    "G = batch5\n",
    "max_length = 512\n",
    "perm = torch.randperm(G.num_graphs)\n",
    "print(perm)\n",
    "print(G)\n",
    "for graph_id in perm:\n",
    "    print(G.get_example(graph_id))\n",
    "# print(G.get_example(0))\n",
    "# print(G.get_example(0).edge_index)\n",
    "# print(G.get_example(2))\n",
    "# print(G.get_example(2).edge_index)\n",
    "# print(G.from_data_list([G.get_example(0), G.get_example(2)]))\n",
    "# print(G.from_data_list([G.get_example(0), G.get_example(2)]).edge_index)\n",
    "\n",
    "accum_nodes = 0\n",
    "data_list = []\n",
    "\n",
    "for graph_id in perm:\n",
    "    data = G.get_example(graph_id)\n",
    "    if accum_nodes + data.num_nodes <= max_length:\n",
    "        accum_nodes += data.num_nodes\n",
    "        data_list.append(data)\n",
    "\n",
    "sub_G = G.from_data_list(data_list)\n",
    "\n",
    "print('Subgraph', sub_G)\n",
    "# perm = torch.randperm(G.num_nodes)\n",
    "# print(G.x)\n",
    "# print(G.edge_index)\n",
    "# idx = perm[:max_length]\n",
    "# print(idx)\n",
    "# sub_G = G.subgraph(idx)\n",
    "print(sub_G.x)\n",
    "print(sub_G.edge_index)\n",
    "# print(sub_G.edge_index.max(), sub_G.edge_index.min())\n",
    "if not sub_G.edge_index.shape[-1]:\n",
    "    # Empty edge index\n",
    "    print(\"Empty edge index !!!\")\n",
    "    s0 = torch.zeros((sub_G.num_nodes, sub_G.num_nodes))\n",
    "else:\n",
    "    s0 = to_dense_adj(sub_G.edge_index, max_num_nodes=len(idx))[0]\n",
    "print(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seq2seq loss\n",
    "# encoder_output2 = seq2seq.encoder(inputs_embeds=x_2.unsqueeze(0)) # for t5\n",
    "max_length = config.max_position_embeddings\n",
    "\n",
    "output = seq2seq(\n",
    "    inputs_embeds=x5_former_embed[:, :max_length, :], \n",
    "    labels=x5_latter_ids[:, :max_length],\n",
    "    decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for bart\n",
    "\n",
    "print(output.keys()) # 'loss', 'logits', 'encoder_last_hidden_state'\n",
    "print(output.loss) # float\n",
    "print(output.logits.shape) # B X T (output seq) X vocab_size\n",
    "print(output.logits)\n",
    "print(output.encoder_last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seq2seq loss\n",
    "# encoder_output2 = seq2seq.encoder(inputs_embeds=x_2.unsqueeze(0)) # for t5\n",
    "max_length = config.max_position_embeddings\n",
    "\n",
    "output2 = seq2seq2(\n",
    "    inputs_embeds=x5_former_embed[:, :max_length, :], \n",
    "    labels=x5_latter_ids[:, :max_length],\n",
    "    decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for bart\n",
    "\n",
    "print(output2.keys()) # 'loss', 'logits', 'encoder_last_hidden_state'\n",
    "print(output2.loss) # float\n",
    "print(output2.logits.shape) # B X T (output seq) X vocab_size\n",
    "print(output2.logits)\n",
    "print(output2.encoder_last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_config = AutoConfig.from_pretrained('xlnet-large-cased')\n",
    "xnlet_base = AutoModel.from_pretrained('xlnet-base-cased')\n",
    "xnlet_large = AutoModel.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "# Modify bart VOCAB\n",
    "xnlet_base.resize_token_embeddings(bgl_node_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xnlet_model decoder\n",
    "# from transformers.models.xlnet.modeling_xlnet\n",
    "\n",
    "decoder_output_final = xnlet_base(\n",
    "    inputs_embeds=x5_, \n",
    "    # decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for xlnet\n",
    "\n",
    "print(decoder_output_final.keys()) # 'last_hidden_state', 'mems'\n",
    "print(decoder_output_final.last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_config = AutoConfig.from_pretrained('gpt2')\n",
    "gpt2_model = AutoModel.from_pretrained('gpt2')\n",
    "\n",
    "# Modify bart VOCAB\n",
    "gpt2_model.resize_token_embeddings(bgl_node_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpe = nn.Embedding(gpt2_config.max_position_embeddings, gpt2_config.hidden_size)\n",
    "position_ids = torch.arange(0, batch5.num_graphs, dtype=torch.long)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, batch5.num_graphs)\n",
    "print(position_ids)\n",
    "graph_embed = wpe(position_ids).squeeze(0)\n",
    "print(graph_embed.shape)\n",
    "\n",
    "node_embed = torch.stack([graph_embed[graphid] for graphid in batch5.batch], dim=0) # |V| X 1024\n",
    "print(node_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model.resize_token_embeddings(131313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '102.10.67.322 is an IP address.'\n",
    "ids1 = bert_tokenizer.encode(text, add_special_tokens=False)\n",
    "ids2 = bert_tokenizer(text, add_special_tokens=False)\n",
    "\n",
    "print(ids1)\n",
    "print(ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.embeddings.word_embeddings.weight[ids1].sum(dim=0).detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.bert.modeling_bert\n",
    "\n",
    "encoder_output_bert = bert_model.encoder(\n",
    "    x5_, \n",
    "    # decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for xlnet\n",
    "\n",
    "print(decoder_output_final.keys()) # 'last_hidden_state', 'mems'\n",
    "print(decoder_output_final.last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.gpt2.modeling_gpt2\n",
    "\n",
    "max_length = gpt2_config.max_position_embeddings\n",
    "\n",
    "decoder_output_gpt2 = gpt2_model(\n",
    "    inputs_embeds=x5_[:, :max_length, :], \n",
    "    # decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for gpt2\n",
    "\n",
    "print(decoder_output_gpt2.keys()) # 'last_hidden_state', 'past_key_values'\n",
    "print(decoder_output_gpt2.last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequence\n",
    "encoder_output2 = seq2seq2.model.encoder(\n",
    "    inputs_embeds=x5_[:, :max_length, :],\n",
    ")\n",
    "print(encoder_output2.keys())\n",
    "print(encoder_output2.last_hidden_state.shape)\n",
    "print(encoder_output2.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequence (BERT)\n",
    "encoder_output3 = plm.encoder(\n",
    "    hidden_states=x5_[:, :max_length, :], \n",
    ")\n",
    "print(encoder_output3.keys())\n",
    "print(encoder_output3.last_hidden_state.shape)\n",
    "print(encoder_output3.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN decode (reconstruction)\n",
    "pair_max, _ = batch5.edge_index.max(dim=0)\n",
    "cutted_edge_index = batch5.edge_index.T[pair_max < max_length].T\n",
    "\n",
    "hidden = encoder_output.last_hidden_state.squeeze(0) # |V_cute| X 1024\n",
    "x_recover = attr_decoder(x=hidden.detach().cpu(), edge_index=cutted_edge_index)\n",
    "\n",
    "# Decode adjacency matrix\n",
    "h_ = struct_decoder(hidden.detach().cpu(), cutted_edge_index)\n",
    "s_ = h_ @ h_.T\n",
    "\n",
    "print(x_recover.shape, s_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistic graph number of nodes\n",
    "from tqdm import tqdm\n",
    "\n",
    "node_nums = []\n",
    "for i, ins in tqdm(enumerate(bgl_node_data.graph_stats)):\n",
    "    node_set = set([x[1] for x in ins['nodes']])\n",
    "    node_nums.append(len(node_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "node_nums = np.array(node_nums)\n",
    "hundred_rate = sum(node_nums > 128)/len(node_nums)\n",
    "print('{:4f}% of graphs with more than 128 nodes'.format(hundred_rate*100))\n",
    "thousand_rate = sum(node_nums > 1024)/len(node_nums)\n",
    "print('{:4f}% of graphs with more than 1024 nodes'.format(thousand_rate*100))\n",
    "two_thousand_rate = sum(node_nums > 2048)/len(node_nums)\n",
    "print('{:4f}% of graphs with more than 2048 nodes'.format(two_thousand_rate*100))\n",
    "\n",
    "node_dist = Counter(node_nums)\n",
    "# print(node_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZE_PATTERN = ' |(=)|(:) |([()])|(,) |([\\[\\]])|([{}])|([<>])|(\\.) |(\\.$)'\n",
    "# s = \"Power Good signal deactivated: R73-M1-N5. A service action may be required.\"\n",
    "s = \"monitor caught java.lang.IllegalStateException: while executing I2C Operation caught java.net.SocketException: Broken pipe and is stopping\"\n",
    "words = list(filter(None, re.split(TOKENIZE_PATTERN, s)))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIT Node Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from argparse import Namespace\n",
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "\n",
    "root = '/nfs/intern_data/yufli/dataset/AIT/seq2seq-node-0.5min-template'\n",
    "\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'ae-gcnae'}\n",
    "\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "ait_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "ait_node_loader = DataLoader(ait_node_data, batch_size=8, shuffle=False)\n",
    "b0 = next(iter(ait_node_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Transformer forward\n",
    "\n",
    "# Graph encode\n",
    "e_ = gcn_encoder(b0.x, b0.edge_index) # |V| X E\n",
    "\n",
    "# Add position embedding (graph-level)\n",
    "wpe = nn.Embedding(gpt2_config.max_position_embeddings, gpt2_config.hidden_size)\n",
    "position_ids = torch.arange(0, b0.num_graphs, dtype=torch.long)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, b0.num_graphs) # |G|\n",
    "graph_embed = wpe(position_ids).squeeze(0) # |G| X E\n",
    "node_embed = torch.stack([graph_embed[graphid] for graphid in b0.batch], dim=0) # |V| X E\n",
    "e_ = e_ + node_embed # |V| X E\n",
    "\n",
    "# GPT2 LM\n",
    "outputs = gpt2_lm_model(\n",
    "    inputs_embeds=e_.unsqueeze(0),\n",
    "    labels=b0.ids.unsqueeze(0),\n",
    ")\n",
    "print(outputs.keys()) # ['loss', 'logits', 'past_key_values', 'hidden_states']\n",
    "print(outputs.loss)\n",
    "last_hidden_state = outputs.hidden_states[-1].squeeze(0) # |V| X E\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/BART_seq2seq/ait/10-shot-0'\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = 0\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# seq2seq = seq2seq\n",
    "\n",
    "# def get_attribute(examples):\n",
    "\n",
    "#     all_graph_ids = []\n",
    "#     for i, node_pair_list in enumerate(examples['nodes']):\n",
    "        \n",
    "#         ent_tag_dict = {x[1]:x[0] for x in node_pair_list} # {entity: tag}\n",
    "#         # print(ent_tag_dict)\n",
    "\n",
    "#         # For each graph, gets template list\n",
    "#         graph_templates = []\n",
    "#         for ent, tag in ent_tag_dict.items():\n",
    "#             # print('ent {} tag {}'.format(ent, tag))\n",
    "#             # First get template\n",
    "#             if tag == 'event':\n",
    "#                 template = ent + ' is an event ID .'\n",
    "#             elif tag == 'component':\n",
    "#                 template = ent + ' is a log message component .'\n",
    "#             else:\n",
    "#                 template = ent + LABEL2TEMPLATE[tag][strategy]\n",
    "            \n",
    "#             graph_templates.append(template)\n",
    "\n",
    "#         # Tokenize and encode each template in a graph\n",
    "#         tokenized_inputs = tokenizer(\n",
    "#             graph_templates,\n",
    "#             padding=False,\n",
    "#             truncation=True,\n",
    "#             max_length=1024,\n",
    "#         )\n",
    "\n",
    "#         # Pad dynamically \n",
    "#         batch = tokenizer.pad(\n",
    "#             tokenized_inputs,\n",
    "#             padding=True,\n",
    "#             max_length=1024,\n",
    "#             pad_to_multiple_of=8,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "\n",
    "#         # Get Encoder outputs\n",
    "#         encoder_outputs = seq2seq.model.encoder(**batch)\n",
    "\n",
    "#         all_graph_ids.append(encoder_outputs.last_hidden_state)\n",
    "\n",
    "#         # print(tokenized_inputs)\n",
    "    \n",
    "#     examples['embeddings'] = all_graph_ids\n",
    "#     return examples\n",
    "\n",
    "\n",
    "# subdata = ait_node_data.graph_stats.select(range(100)).map(\n",
    "#     get_attribute,\n",
    "#     batched=True,\n",
    "#     num_proc=None,\n",
    "#     load_from_cache_file=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from argparse import Namespace\n",
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from node_model import NodeConv, GCN, AENodeConv\n",
    "\n",
    "root = 'dataset/BGL/seq2seq-node-0.5min-template-bertembed'\n",
    "\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'dynamic'}\n",
    "\n",
    "# Define model args\n",
    "in_channels = 768\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "node_loader = DataLoader(node_data, batch_size=4, shuffle=False)\n",
    "b0 = next(iter(node_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.typing import Adj\n",
    "\n",
    "p_a = nn.Parameter(torch.randn(1024))\n",
    "p_b = nn.Parameter(torch.randn(1024))\n",
    "beta = 0.5\n",
    "mu = 0.5\n",
    "gamma = 0.5\n",
    "\n",
    "def score_func(hidden, i, j, weight):\n",
    "    dist = (p_a * hidden[i] + p_b * hidden[j]).norm()**2\n",
    "    print('dist {}, weight {}'.format(dist, weight))\n",
    "    return weight * (beta * (dist - mu)).sigmoid()\n",
    "\n",
    "def neg_sampling(degrees, i, j, s):\n",
    "    # negative sampling\n",
    "    prob_i = degrees[i]/(degrees[i] + degrees[j]) if degrees[i] + degrees[j] else 0\n",
    "    if torch.rand(1) <= prob_i:\n",
    "        # replace node i\n",
    "        i_prime = j\n",
    "        while i_prime == j or s[i_prime, j] != 0:\n",
    "            i_prime = torch.randint(s.size()[0], (1,)).item()\n",
    "        return i_prime, j\n",
    "    else:\n",
    "        # replace node j\n",
    "        j_prime = i\n",
    "        while j_prime == i or s[i, j_prime] != 0:\n",
    "            j_prime = torch.randint(s.size()[0], (1,)).item()\n",
    "        return i, j_prime\n",
    "\n",
    "def margin_loss(hidden, edge_index, s, num_nodes):\n",
    "    degrees = degree(edge_index[0], num_nodes)\n",
    "    scores = []\n",
    "    for i, j in edge_index.T.tolist():\n",
    "        pos_score = score_func(hidden, i, j, s[i, j])\n",
    "        # negative sampling\n",
    "        i_prime, j_prime = neg_sampling(degrees, i, j, s)\n",
    "        neg_score = score_func(hidden, i_prime, j_prime, s[i, j])\n",
    "\n",
    "        if pos_score <= neg_score:\n",
    "            edge_loss = max(0, gamma + pos_score - neg_score)\n",
    "            scores.append(edge_loss)\n",
    "            # edge_loss = F.relu(gamma + pos_score - neg_score)\n",
    "            # print(edge_loss)\n",
    "            # loss += edge_loss\n",
    "            # print(loss)\n",
    "    print(torch.stack(scores))\n",
    "    return sum(scores)\n",
    "\n",
    "def get_nonedge(s: Tensor):\n",
    "    row = []\n",
    "    new_s = s.clone()\n",
    "    for i in range(s.size()[0]):\n",
    "        new_s[i,i] = 1\n",
    "        row.append((new_s[i] == 0).nonzero().squeeze().tolist())\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH\n",
    "print(b0, b0.num_graphs)\n",
    "b0.s = to_dense_adj(b0.edge_index)[0]\n",
    "print('feature', b0.x)\n",
    "print('edge index', b0.edge_index)\n",
    "print('adjacency matrix', b0.s)\n",
    "# margin_loss(b0.x, b0.edge_index, b0.s, b0.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.Tensor([[0, 1, 1, 1], [1, 0, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0]])\n",
    "non_row = get_nonedge(s)\n",
    "print(\"non-adj: \", non_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(s.size()[0]):\n",
    "    print(non_row[i])\n",
    "    if not non_row[i]:\n",
    "        print('All nodes are connected to node {}'.format(i))\n",
    "    else:\n",
    "        print('Node {} is not connected to {}'.format(i, non_row[i]))\n",
    "        if isinstance(non_row[i], list):\n",
    "            print('Randomly select a node from {}'.format(random.choice(non_row[i])))\n",
    "        else:\n",
    "            print('Randomly select a node from {}'.format(non_row[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3787, 0.7466, 1.2546, 1.3946], grad_fn=<MulBackward0>)\n",
      "tensor([0.3780, 0.5822, 1.2237, 1.8091], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# p_a = nn.Parameter(torch.DoubleTensor(768))\n",
    "# p_b = nn.Parameter(torch.DoubleTensor(768))\n",
    "# p_a_ = p_a.unsqueeze(0)\n",
    "# nn.init.xavier_uniform_(p_a_.data, gain=1.414)\n",
    "# p_b_ = p_b.unsqueeze(0)\n",
    "# nn.init.xavier_uniform_(p_b_.data, gain=1.414)\n",
    "p_a = nn.Linear(768, 1, bias=False)\n",
    "p_b = nn.Linear(768, 1, bias=False)\n",
    "\n",
    "weights = torch.tensor([1, 2, 3, 4])\n",
    "hidden = torch.rand(1000, 768) # |V| x d\n",
    "beta = 1.0\n",
    "mu = 0.5\n",
    "gamma = 0.5\n",
    "rows, cols = torch.tensor([10, 39, 277, 391]), torch.tensor([13, 88, 302, 870])\n",
    "new_rows, new_cols = torch.tensor([72, 59, 136, 225]), torch.tensor([334, 866, 899, 993])\n",
    "\n",
    "# pos scores\n",
    "hidden_i = hidden.index_select(0, rows) # |E| x d\n",
    "hidden_j = hidden.index_select(0, cols) # |E| x d\n",
    "# s = p_a.expand_as(hidden_i) * hidden_i + p_b.expand_as(hidden_j) * hidden_j # |E| x d\n",
    "# s = F.dropout(s, 0.5) # |E| x d\n",
    "s = (p_a(hidden_i) + p_b(hidden_j)).squeeze(1) # |E|\n",
    "pos_scores = weights * torch.sigmoid(beta * s - mu) # |E|\n",
    "print(pos_scores)\n",
    "\n",
    "# neg scores\n",
    "hidden_i = hidden.index_select(0, new_rows) # |E| x d\n",
    "hidden_j = hidden.index_select(0, new_cols) # |E| x d\n",
    "# s = p_a.expand_as(hidden_i) * hidden_i + p_b.expand_as(hidden_j) * hidden_j # |E| x d\n",
    "# s = F.dropout(s, 0.5) # |E| x d\n",
    "s = (p_a(hidden_i) + p_b(hidden_j)).squeeze(1) # |E|\n",
    "neg_scores = weights * torch.sigmoid(beta * s - mu) # |E|\n",
    "print(neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9925, 1.9722, 2.9632, 3.9651], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([0.9948, 1.9733, 2.9673, 3.9677], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor(1.9898, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([0.6190, 1.2373, 1.8638, 2.4801])\n"
     ]
    }
   ],
   "source": [
    "print(pos_scores)\n",
    "print(neg_scores)\n",
    "edge_loss = F.relu(gamma + pos_scores - neg_scores)[pos_scores <= neg_scores].sum()\n",
    "print(edge_loss)\n",
    "pos_scores[pos_scores <= neg_scores]\n",
    "scores = weights * torch.sigmoid((hidden_i + hidden_j).mean(dim=1) - mu)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.6845688  0.5002458  0.5925464  0.5670235  0.53375936 0.50311154\n",
      " 0.6268135  0.5859661  0.5556578  0.6602983  0.5734128  0.6909739\n",
      " 0.61061335 0.56913215 0.71732575 0.52564883 0.61638093 0.5242581\n",
      " 0.64076644 0.6044951  0.50302035 0.6802265  0.6458474  0.66379446\n",
      " 0.5427395  0.503417   0.6367918  0.54072493 0.57274914 0.60743284]\n",
      "labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "anomaly index:  [27, 28, 29]\n",
      "normal index:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "sampled normal index:  [22, 15, 9]\n",
      "new index: [9, 15, 22, 27, 28, 29]\n",
      "new scores:  [0.6602983  0.52564883 0.6458474  0.54072493 0.57274914 0.60743284]\n",
      "new labels:  [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "scores = torch.rand(30).sigmoid().numpy()\n",
    "labels = np.array([0]*27 + [1]*3)\n",
    "print('scores: ', scores)\n",
    "print('labels: ', labels)\n",
    "anomaly_ids = np.where(labels == 1)[0].tolist()\n",
    "print('anomaly index: ', anomaly_ids)\n",
    "normal_ids = np.where(labels == 0)[0].tolist()\n",
    "print('normal index: ', normal_ids)\n",
    "num_anomalies = sum(labels)\n",
    "if len(normal_ids) > num_anomalies:\n",
    "    subnormal_ids = random.sample(normal_ids, num_anomalies)\n",
    "    print('sampled normal index: ', subnormal_ids)\n",
    "    new_ids = sorted(anomaly_ids + subnormal_ids)\n",
    "    print('new index:', new_ids)\n",
    "    new_scores = scores[new_ids]\n",
    "    new_labels = labels[new_ids]\n",
    "    print('new scores: ', new_scores)\n",
    "    print('new labels: ', new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph in BATCH\n",
    "for i in range(b0.num_graphs): \n",
    "    print(b0[i])\n",
    "\n",
    "i = 0\n",
    "print('graph #{}: {}'.format(i, b0[i]))\n",
    "print('# of nodes', b0[i].num_nodes)\n",
    "print('feature', b0[i].x)\n",
    "print('edge index', b0[i].edge_index)\n",
    "print('adjacency matrix', to_dense_adj(b0[i].edge_index)[0])\n",
    "print('degree', degree(b0[i].edge_index[0], b0[i].num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree(b0.edge_index[0], b0.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b0.s)\n",
    "print(b0.s.size()[0])\n",
    "print(b0.s[1])\n",
    "print(b0.s[1].nonzero().size()[0])\n",
    "print(b0.s[:,1])\n",
    "print(b0.s[:,1].nonzero().size()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sock Shop Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dataset/sockshop/sockshop.csv'\n",
    "df = pd.read_csv(file) # 49442 rows\n",
    "df['ParameterList'] = [[] for _ in range(len(df))] # create entity list\n",
    "# df['NerList'] = [[] for _ in range(len(df))] # create ner list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    df['log'].str.contains(\"item|customer|user\", na = False)\n",
    "].to_csv('dataset/sockshop/database_customer_v3.csv') # select rows with item & customer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dataset/sockshop/database_customer_v3.csv'\n",
    "df = pd.read_csv(file) # 14673 rows X 8 columns\n",
    "# df.drop(['Unnamed: 0', 'message'], axis=1, inplace=True)\n",
    "df.rename(columns={'log': 'Content', '@timestamp': 'Timestamp'}, inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dataset/sockshop/regex-node-5s-template-bartembed/raw/graph.json', 'r') as f:\n",
    "    for line in f:\n",
    "        instance = json.load(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_json('dataset/sockshop/regex-node-5s-template-bartembed/raw/graph.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "be46a5aa612bad42a8f9c64207b0b4a7e1730d2f91497a6197c6ec126dab2062"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
