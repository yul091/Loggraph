{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset, \n",
    "    load_metric, \n",
    "    load_from_disk, \n",
    "    concatenate_datasets\n",
    ")\n",
    "# from transformers import (\n",
    "#     AutoConfig,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModel,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "# )\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference & Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_structure_data(server):\n",
    "    \"\"\"Get labels for the structured data.\"\"\"\n",
    "    dev_to_path = {\n",
    "        k: [os.path.join(AIT_DATA_ROOT, 'data', v, server), os.path.join(AIT_DATA_ROOT, 'labels', v, server)]\n",
    "        for k,v in AIT_NAME_DICT.items()\n",
    "    }\n",
    "    label_df = {'log': [], 'label': [], 'device': []}\n",
    "    for dev, path_pair in dev_to_path.items():\n",
    "        label_df['log'].extend(pd.read_csv(path_pair[0], names=['log'])['log'].to_list())\n",
    "        label_col = pd.read_csv(path_pair[1], header=None)\n",
    "        label_df['label'].extend([(str(l1), str(l2)) for l1, l2 in zip(label_col[0], label_col[1])])\n",
    "        label_df['device'].extend([dev for _ in range(len(label_col))])\n",
    "\n",
    "    label_df = pd.DataFrame(label_df)\n",
    "    temp_dir = 'dataset/slogert/' + server + '/2-logpai/'\n",
    "    struct_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('structured.csv')]\n",
    "    struct_df = pd.read_csv(struct_files[0], delimiter=\",\")\n",
    "    if len(struct_files) > 1:\n",
    "        for file_path in struct_files[1:]:\n",
    "            struct_df = struct_df.append(pd.read_csv(file_path, delimiter=\",\"), ignore_index=True)\n",
    "    struct_df['Label'] = label_df['label']\n",
    "    print('label set:', set(tuple(x) for x in struct_df['Label']))\n",
    "    struct_dataset = Dataset.from_dict(struct_df)\n",
    "    print('structure', struct_dataset)\n",
    "    print('example[0]:', struct_dataset[0])\n",
    "    return struct_df, struct_dataset\n",
    "\n",
    "# Predict and store results on templates\n",
    "def gen_entities_from_patterns(struct_df, struct_dataset):\n",
    "    preds_pattern = {}\n",
    "    for eventID, insIDs in struct_df.groupby(['EventId']).groups.items():\n",
    "        instance = struct_dataset[int(insIDs[0])] # pick the first instance of each group\n",
    "        print(instance['Content'])\n",
    "        preds = prediction(instance['Content'])  # token classification\n",
    "        print('Pred:', preds)\n",
    "        entities = list(get_entities_bio(preds)) # merge tokens within the same entity\n",
    "        entities.sort(key=lambda x: x[1])\n",
    "        # print('Extracted entities:', entities)\n",
    "        input_tokens = list(filter(None, re.split(TOKENIZE_PATTERN, instance['Content'])))  \n",
    "        ent_list = [(tag, ' '.join(input_tokens[start:end+1])) for (tag, start, end) in entities]\n",
    "        print('Extracted entities:', ent_list)\n",
    "        preds_pattern[eventID] = entities\n",
    "\n",
    "    preds = []\n",
    "    for i, instance in enumerate(struct_dataset):\n",
    "        ent_ids = preds_pattern[instance['EventId']]\n",
    "        log = instance['Content']\n",
    "        input_tokens = list(filter(None, re.split(TOKENIZE_PATTERN, log))) \n",
    "        ent_list = [(tag, ' '.join(input_tokens[start:end+1])) for (tag, start, end) in ent_ids]\n",
    "        preds.append(ent_list)\n",
    "    \n",
    "    struct_dataset = struct_dataset.add_column('Preds', preds)\n",
    "    return preds_pattern, struct_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auth.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['LineId', 'Device', 'Month', 'Date', 'Time', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds'],\n",
      "    num_rows: 4610\n",
      "})\n",
      "example[0]: {'LineId': 1, 'Device': 'mail.cup.com', 'Month': 'Feb', 'Date': 29, 'Time': '00:09:01', 'Type': 'mail-0', 'Component': 'CRON[32002]', 'Content': 'pam_unix(cron:session): session opened for user root by (uid=0)', 'EventId': 'e962e6c3', 'EventTemplate': 'pam_unix(cron:<*>): session opened for user <*> by (uid=<*>)', 'ParameterList': \"['session', 'root', '0']\", 'Label': ['0', '0'], 'Preds': [['path', 'pam_unix'], ['user', 'root']]}\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DIR = 'dataset/processed'\n",
    "server0 = 'auth.log' # Handle auth.log \n",
    "out_file0 = 'auth-log-dataset-processed.json'\n",
    "\n",
    "if not os.path.isdir(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR)\n",
    "\n",
    "out_path0 = os.path.join(PROCESSED_DIR, out_file0)\n",
    "if os.path.exists(out_path0):\n",
    "    struct_dataset0 = load_from_disk(out_path0)\n",
    "else:\n",
    "    struct_df0, struct_dataset0 = gen_structure_data(server0)\n",
    "    preds_pattern0, struct_dataset0 = gen_entities_from_patterns(struct_df0, struct_dataset0)\n",
    "    struct_dataset0.save_to_disk(out_path0)\n",
    "\n",
    "print(struct_dataset0)\n",
    "print(\"example[0]:\", struct_dataset0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daemon.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['LineId', 'Device', 'Month', 'Date', 'Time', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds'],\n",
      "    num_rows: 6121\n",
      "})\n",
      "example[0]: {'LineId': 1, 'Device': 'mail.cup.com', 'Month': 'Feb', 'Date': 29, 'Time': '00:09:19', 'Type': 'mail-0', 'Component': 'systemd[1]', 'Content': 'Starting Clean php session files...', 'EventId': '73f564e0', 'EventTemplate': 'Starting Clean php session files...', 'ParameterList': '[]', 'Label': ['0', '0'], 'Preds': []}\n"
     ]
    }
   ],
   "source": [
    "server1 = 'daemon.log' # Handle auth.log\n",
    "out_file1 = 'daemon-log-dataset-processed.json'\n",
    "out_path1 = os.path.join(PROCESSED_DIR, out_file1)\n",
    "\n",
    "if os.path.exists(out_path1):\n",
    "    struct_dataset1 = load_from_disk(out_path1)\n",
    "else:\n",
    "    struct_df1, struct_dataset1 = gen_structure_data(server1)\n",
    "    preds_pattern1, struct_dataset1 = gen_entities_from_patterns(struct_df1, struct_dataset1)\n",
    "    struct_dataset1.save_to_disk(out_path1)\n",
    "\n",
    "print(struct_dataset1)\n",
    "print(\"example[0]:\", struct_dataset1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mail.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['LineId', 'Device', 'Month', 'Date', 'Time', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds'],\n",
      "    num_rows: 360468\n",
      "})\n",
      "example[0]: {'LineId': 1, 'Device': 'mail.onion.com', 'Month': 'Mar', 'Date': 3, 'Time': '08:06:22', 'Type': 'mail', 'Component': 'dovecot', 'Content': 'imap(idella): Logged out in=211 out=5132', 'EventId': '7edba00f', 'EventTemplate': 'imap(<*>): Logged out in=<*> out=<*>', 'ParameterList': \"['idella', '211', '5132']\", 'Label': ['0', '0'], 'Preds': [['user', 'idella'], ['port', '5132']]}\n"
     ]
    }
   ],
   "source": [
    "server2 = 'mail.log' # Handle auth.log \n",
    "out_file2 = 'mail-log-dataset-processed.json'\n",
    "out_path2 = os.path.join(PROCESSED_DIR, out_file2)\n",
    "\n",
    "if os.path.exists(out_path2):\n",
    "    struct_dataset2 = load_from_disk(out_path2)\n",
    "else:\n",
    "    struct_df2, struct_dataset2 = gen_structure_data(server2)\n",
    "    preds_pattern2, struct_dataset2 = gen_entities_from_patterns(struct_df2, struct_dataset2)\n",
    "    struct_dataset2.save_to_disk(out_path2)\n",
    "\n",
    "print(struct_dataset2)\n",
    "print(\"example[0]:\", struct_dataset2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['LineId', 'Device', 'Month', 'Date', 'Time', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds'],\n",
      "    num_rows: 111577\n",
      "})\n",
      "example[0]: {'LineId': 1, 'Device': 'mail.cup.com', 'Month': 'Feb', 'Date': 29, 'Time': '00:00:12', 'Type': 'mail-0', 'Component': 'HORDE', 'Content': '[horde] Login success for karri to horde (192.168.10.190) [pid 31779 on line 163 of \"/var/www/mail.cup.com/login.php\"]', 'EventId': 'c9f3df73', 'EventTemplate': '[<*>] Login success for <*> <*> <*> <*> [pid <*> on line <*> of \"/<*>\"]', 'ParameterList': \"['horde', 'karri', 'to horde (192.168.10.190)', '31779', '163', 'var/www/mail.cup.com/login.php']\", 'Label': ['0', '0'], 'Preds': [['server', 'horde'], ['user', 'karri'], ['server', 'horde'], ['ip', '192.168.10.190'], ['pid', '31779']]}\n"
     ]
    }
   ],
   "source": [
    "server3 = 'messages' # Handle auth.log \n",
    "out_file3 = 'messages-dataset-processed.json'\n",
    "out_path3 = os.path.join(PROCESSED_DIR, out_file3)\n",
    "\n",
    "if os.path.exists(out_path3):\n",
    "    struct_dataset3 = load_from_disk(out_path3)\n",
    "else:\n",
    "    struct_df3, struct_dataset3 = gen_structure_data(server3)\n",
    "    preds_pattern3, struct_dataset3 = gen_entities_from_patterns(struct_df3, struct_dataset3)\n",
    "    struct_dataset3.save_to_disk(out_path3)\n",
    "\n",
    "print(struct_dataset3)\n",
    "print(\"example[0]:\", struct_dataset3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syslog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['LineId', 'Device', 'Month', 'Date', 'Time', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds'],\n",
      "    num_rows: 480547\n",
      "})\n",
      "example[0]: {'LineId': 1, 'Device': 'mail.spiral.com', 'Month': 'Mar', 'Date': 5, 'Time': '22:01:32', 'Type': 'mail', 'Component': 'dovecot', 'Content': 'imap-login: Login: user=<gilberte>, method=PLAIN, rip=127.0.0.1, lip=127.0.0.1, mpid=30824, secured, session=<3oP8rSKgHoZ/AAAB>', 'EventId': '19d70957', 'EventTemplate': 'imap-login: Login: user=<<*>>, method=<*>, rip=<*>, lip=<*>, mpid=<*>, secured, <*>', 'ParameterList': \"['<gilberte>', 'PLAIN', '127.0.0.1', '127.0.0.1', '30824', 'session=<3oP8rSKgHoZ/AAAB>']\", 'Label': ['0', '0'], 'Preds': [['user', 'gilberte'], ['ip', '127.0.0.1'], ['ip', '127.0.0.1'], ['pid', '30824'], ['session', '3oP8rSKgHoZ/AAAB']]}\n"
     ]
    }
   ],
   "source": [
    "server4 = 'syslog' # Handle auth.log \n",
    "out_file4 = 'syslog-dataset-processed.json'\n",
    "out_path4 = os.path.join(PROCESSED_DIR, out_file4)\n",
    "\n",
    "if os.path.exists(out_path4):\n",
    "    struct_dataset4 = load_from_disk(out_path4)\n",
    "else:\n",
    "    struct_df4, struct_dataset4 = gen_structure_data(server4)\n",
    "    preds_pattern4, struct_dataset4 = gen_entities_from_patterns(struct_df4, struct_dataset4)\n",
    "    struct_dataset4.save_to_disk(out_path4)\n",
    "\n",
    "print(struct_dataset4)\n",
    "print(\"example[0]:\", struct_dataset4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['LineId', 'Device', 'Month', 'Date', 'Time', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds'],\n",
      "    num_rows: 111579\n",
      "})\n",
      "example[0]: {'LineId': 1, 'Device': 'mail.insect.com', 'Month': 'Mar', 'Date': 5, 'Time': '17:13:31', 'Type': 'mail', 'Component': 'HORDE', 'Content': '[nag] PHP ERROR: Declaration of Horde_Form_Type_country::init($prompt = NULL) should be compatible with Horde_Form_Type_enum::init($values, $prompt = NULL) [pid 10486 on line 0 of \"/usr/share/php/Horde/Form/Type.php\"]', 'EventId': 'c7a9a5e4', 'EventTemplate': '[<*>] PHP ERROR: Declaration of Horde_Form_Type_country::<*>($prompt = NULL) should be compatible with Horde_Form_Type_enum::<*>($values, $prompt = NULL) [pid <*> on line <*> of \"/<*>\"]', 'ParameterList': \"['nag', 'init', 'init', '10486', '0', 'usr/share/php/Horde/Form/Type.php']\", 'Label': ['0', '0'], 'Preds': [['pid', '10486'], ['pid', '0']]}\n"
     ]
    }
   ],
   "source": [
    "server5 = 'user.log' # Handle user.log \n",
    "out_file5 = 'user-dataset-processed.json'\n",
    "out_path5 = os.path.join(PROCESSED_DIR, out_file5)\n",
    "\n",
    "if os.path.exists(out_path5):\n",
    "    struct_dataset5 = load_from_disk(out_path5)\n",
    "else:\n",
    "    struct_df5, struct_dataset5 = gen_structure_data(server5)\n",
    "    preds_pattern5, struct_dataset5 = gen_entities_from_patterns(struct_df5, struct_dataset5)\n",
    "    struct_dataset5.save_to_disk(out_path5)\n",
    "\n",
    "print(struct_dataset5)\n",
    "print(\"example[0]:\", struct_dataset5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Grouped Datasets (by time interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole Dataset({\n",
      "    features: ['LineId', 'Device', 'Type', 'Component', 'Content', 'EventId', 'EventTemplate', 'ParameterList', 'Label', 'Preds', 'Timestamp', 'Datetime'],\n",
      "    num_rows: 1074902\n",
      "})\n",
      "# anomalies 45651\n",
      "# events 287\n",
      "average length 159.32038176503534\n"
     ]
    }
   ],
   "source": [
    "# Combine Day-Month-Time into numerical values\n",
    "def time2value(examples):\n",
    "    stamplist, datelist = [], []\n",
    "    for i, hms_time in enumerate(examples['Time']):\n",
    "        mon = examples['Month'][i]\n",
    "        day = examples['Date'][i]\n",
    "        mon_number = datetime.strptime(mon, '%b').month\n",
    "        date_string = f'2020-{mon_number}-{day} {hms_time}' # standard datetime string\n",
    "        datelist.append(date_string)\n",
    "        dt = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\").timetuple()\n",
    "        allsecs = time.mktime(dt)\n",
    "        stamplist.append(allsecs) # timestamp (numerical) values\n",
    "\n",
    "    examples['Timestamp'] = stamplist\n",
    "    examples['Datetime'] = datelist\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Merge all hosts and sort logs by Timestamp\n",
    "PROCESSED_DIR = 'dataset/AIT-LDS-v1_1/processed'\n",
    "out_file = 'whole-dataset-processed.json'\n",
    "out_path = os.path.join(PROCESSED_DIR, out_file)\n",
    "if os.path.exists(out_path):\n",
    "    whole_dataset = load_from_disk(out_path)\n",
    "else:\n",
    "    whole_dataset = concatenate_datasets(\n",
    "        [struct_dataset0, struct_dataset1, struct_dataset2, struct_dataset3, struct_dataset4, struct_dataset5]\n",
    "    )\n",
    "    whole_dataset = whole_dataset.map(time2value, batched=True, load_from_cache_file=None)\n",
    "    whole_dataset = whole_dataset.remove_columns(['Month', 'Date', 'Time'])\n",
    "    whole_dataset = whole_dataset.sort('Timestamp') # sort by timestamp\n",
    "    whole_dataset.save_to_disk(out_path)\n",
    "\n",
    "print('whole', whole_dataset)\n",
    "# print(\"example[0]:\", whole_dataset[0])\n",
    "whole_df = whole_dataset.to_pandas()\n",
    "whole_df.head()\n",
    "\n",
    "# Get statistics\n",
    "df_labels = whole_df.Label.apply(lambda x: 0 if set(x) == set('0') else 1)\n",
    "print('# anomalies', sum(df_labels))\n",
    "print('# events', len(whole_df.EventId.unique()))\n",
    "print('average length', np.mean([len(row.Content) for idx, row in whole_df.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1075/1075 [00:18<00:00, 59.68ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "740837983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dataset.to_json('dataset/AIT/AIT.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Graphs (by time interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitbyinterval(df, interval):\n",
    "    new_df = df.copy(deep=True)\n",
    "    new_df['Datetime'] = pd.to_datetime(new_df['Datetime'], errors='coerce')\n",
    "    period = new_df.groupby(pd.Grouper(key='Datetime', freq=interval)).ngroup()\n",
    "    new_df['Period'] = np.char.add('period_', (pd.factorize(period)[0]).astype(str))\n",
    "    return new_df\n",
    "\n",
    "def gen_period_graph(dataframe, root_dir):\n",
    "    num_unique_intervals = len(dataframe.Period.unique())\n",
    "    graph_stats = []\n",
    "    stats_file = os.path.join(root_dir, 'graph_stats.json')\n",
    "    invalid_entities = set([('ip', '127.0.0.1'), ('pid', '0')]) # filtering invalid entities\n",
    "    # num_unique_intervals\n",
    "    for period_id in tqdm(range(num_unique_intervals)):\n",
    "        df_sub = dataframe.loc[dataframe.Period == f'period_{period_id}'] # get subdf for current group\n",
    "        num_logs = len(df_sub) # count number of logs in current group\n",
    "        num_anomaly = len(df_sub.loc[df_sub.Label.apply(lambda x: '0' not in x)]) # count anomaly logs in this group\n",
    "        anomaly_rate = num_anomaly/num_logs if num_logs else 0 # calculate anomaly rate\n",
    "        # Store graph\n",
    "        out_normal_file = os.path.join(root_dir, f'period_{period_id}.txt')\n",
    "        gen_file = os.path.join(root_dir, f'period_{period_id}.html')\n",
    "        # Generate entity-tag pairs\n",
    "        with open(out_normal_file, 'w') as f:\n",
    "            # for entities in df_sub['Preds']: # for each log \n",
    "            #     for i in range(len(entities)-1):\n",
    "            #         for j in range(i+1, len(entities)):\n",
    "            #             # tag0, entity0, tag1, entity1\n",
    "            #             f.write(entities[i][0]+'\\t'+entities[i][1]+'\\t'+entities[j][0]+'\\t'+entities[j][1])\n",
    "            #             f.write('\\n')\n",
    "            for idx in range(num_logs):\n",
    "                instance = df_sub.iloc[idx]\n",
    "                # Connect eventID to Component\n",
    "                component = re.split('([\\[\\]])', instance.Component)[0] # remove [XXXX] part\n",
    "                f.write('event' + '\\t' + instance.EventId + '\\t' + 'component' + '\\t' + component)\n",
    "                f.write('\\n')\n",
    "                # Connect Component to Device\n",
    "                f.write('component' + '\\t' + component + '\\t' + 'device' + '\\t' + instance.Device)\n",
    "                f.write('\\n')\n",
    "                # Connect eventID to each entity\n",
    "                for entity in instance.Preds:\n",
    "                    if tuple(entity) not in invalid_entities: # valid entity pair\n",
    "                        f.write('event' + '\\t' + instance.EventId + '\\t' + entity[0] + '\\t' + entity[1])\n",
    "                        f.write('\\n')\n",
    "                \n",
    "        f.close()\n",
    "        # Generate graph visualization file\n",
    "        visualize(out_normal_file, gen_file)\n",
    "        ent_count, edge_count = read_hyper(out_normal_file)\n",
    "        num_edges = len(edge_count)\n",
    "        num_nodes = len(ent_count)\n",
    "        avg_degree = num_edges/num_nodes if num_nodes else 0\n",
    "        avg_degree = math.ceil(avg_degree*10000)/10000\n",
    "        stats = {'graph_ID': period_id, '#nodes': num_nodes, '#edges': num_edges, 'degree': avg_degree, '#log': num_logs, '#anomaly': num_anomaly, 'anomaly_rate': anomaly_rate}\n",
    "        graph_stats.append(stats)\n",
    "\n",
    "    with open(stats_file, 'w') as f:\n",
    "        for stats in graph_stats: # for each log \n",
    "            f.write(json.dumps(stats))\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by time interval\n",
    "# interval = '10min'\n",
    "# interval = '5min'\n",
    "interval = '2min'\n",
    "grouped_df = splitbyinterval(whole_df, interval)\n",
    "demo_dir = os.path.join('dataset/new_graph', interval)\n",
    "\n",
    "if not os.path.exists(demo_dir):\n",
    "    os.makedirs(demo_dir)\n",
    "\n",
    "# Generate subgraphs based on time interval\n",
    "# gen_period_graph(grouped_df, demo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df_path = '/nfs/intern_data/yufli/dataset/AIT_0.5min_df.csv'\n",
    "data_df = pd.read_csv(common_df_path, engine='c', na_filter=False, memory_map=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDFS dataset demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loglizer.models import *\n",
    "from loglizer import dataloader, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_log = 'dataset/HDFS-demo/HDFS_100k.log_structured.csv' # The structured log file\n",
    "label_file = 'dataset/HDFS-demo/anomaly_label.csv' # The anomaly label file\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_HDFS(\n",
    "    struct_log,\n",
    "    label_file=label_file,\n",
    "    window='session', \n",
    "    train_ratio=0.5,\n",
    "    split_type='uniform'\n",
    ")\n",
    "\n",
    "feature_extractor = preprocessing.FeatureExtractor()\n",
    "x_train = feature_extractor.fit_transform(x_train, term_weighting='tf-idf')\n",
    "x_test = feature_extractor.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()\n",
    "model.fit(x_train, y_train)\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "\n",
    "print('Test validation:')\n",
    "precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on HDFS benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'dataset/HDFS-demo/HDFS_100k.log_structured.csv' # the structured log file\n",
    "label_file = 'dataset/HDFS-demo/anomaly_label.csv' # the anomaly label file\n",
    "struct_df = pd.read_csv(log_file, na_filter=False, memory_map=True)\n",
    "struct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import add_preds_to_df\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "bart_model = AutoModel.from_pretrained(f\"results/BART_seq2seq/10-shot-0\")\n",
    "add_preds_to_df(struct_df, 'regex', bart_model, tokenizer, 0)\n",
    "\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get blockId and corresponding logs\n",
    "print(\"Getting BlockIDs and Logs!!! Total number of logs: {}\".format(struct_df.shape[0]))\n",
    "data_dict = OrderedDict()\n",
    "for idx, row in tqdm(struct_df.iterrows()):\n",
    "    blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "    blkId_set = set(blkId_list)\n",
    "    for blk_Id in blkId_set:\n",
    "        if not blk_Id in data_dict:\n",
    "            data_dict[blk_Id] = defaultdict(list)\n",
    "            data_dict[blk_Id]['BlockId'] = blk_Id\n",
    "        for col in struct_df.columns:\n",
    "            data_dict[blk_Id][col].append(row[col])\n",
    "\n",
    "data_df = pd.DataFrame(data_dict.values())\n",
    "\n",
    "# Add labels to each block \n",
    "label_data = pd.read_csv(label_file, engine='c', na_filter=False, memory_map=True)\n",
    "label_data = label_data.set_index('BlockId')\n",
    "label_dict = label_data['Label'].to_dict()\n",
    "data_df['Label'] = data_df['BlockId'].apply(lambda x: 1 if label_dict[x] == 'Anomaly' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "print(\"Splitting graph datasets!!!\")\n",
    "num_total = data_df.shape[0]\n",
    "normal_samples = data_df[data_df.Label == 0]\n",
    "anomaly_samples = data_df[data_df.Label == 1]\n",
    "num_normal = normal_samples.shape[0]\n",
    "num_anomaly = anomaly_samples.shape[0]\n",
    "anomaly_rate = num_anomaly/num_total if num_total else 0\n",
    "\n",
    "train_df, test_normal_df = train_test_split(normal_samples, test_size=0.2, random_state=seed)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=seed)\n",
    "test_df = pd.concat([anomaly_samples, test_normal_df], ignore_index=True)\n",
    "test_anomaly_rate = num_anomaly/test_df.shape[0] if test_df.shape[0] else 0\n",
    "\n",
    "print(\"Total number of graphs: {}, normal graphs: {}, anomaly graphs: {}, anomaly ratio: {:.4f}\".format(\n",
    "    num_total, num_normal, num_anomaly, anomaly_rate))\n",
    "print(\"Train data size: {}, validation data size: {}, test data size: {}, test anomaly ratio: {:.4f}\".format(\n",
    "    train_df.shape[0], val_df.shape[0], test_df.shape[0], test_anomaly_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch geometric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset import HDFSDataset\n",
    "\n",
    "# Test graph dataset\n",
    "root = 'dataset/HDFS/regex'\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "tag2id['date'] = len(tag2id) # for hdfs dataset\n",
    "# df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    plm=plm,\n",
    "    ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    ")\n",
    "hdfs_data = HDFSDataset(root, hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics (using huggingface dataset)\n",
    "hdfs_stats_data = hdfs_data.graph_stats\n",
    "print(hdfs_stats_data)\n",
    "# Retrieve normal and anomaly indices\n",
    "anomaly_ids = [i for i, y in enumerate(hdfs_stats_data['label']) if y == 1]\n",
    "normal_ids = [i for i, y in enumerate(hdfs_stats_data['label']) if y == 0]\n",
    "print(\"#anomly {}, #normal {}\".format(len(anomaly_ids), len(normal_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "k = 10\n",
    "# randomly chose k normal and anomaly graphs\n",
    "a_sub_ids = random.sample(anomaly_ids, k=k)\n",
    "for idx in a_sub_ids:\n",
    "    hdfs_data._visualize(idx)\n",
    "n_sub_ids = random.sample(normal_ids, k=k)\n",
    "for idx in n_sub_ids:\n",
    "    hdfs_data._visualize(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGL dataset few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'dataset/BGL/BGL.log_structured.csv' # the structured log file\n",
    "bgl_df = pd.read_csv(log_file, na_filter=False, memory_map=True)\n",
    "bgl_df['Tag'] = bgl_df['Label'].apply(lambda x: 0 if x == '-' else 1)\n",
    "bgl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import splitbyinterval\n",
    "\n",
    "bgl_demo_df = bgl_df.sample(frac=0.1) # 471349\n",
    "grouped_df = splitbyinterval(bgl_demo_df, interval='2min')\n",
    "data_dict = OrderedDict()\n",
    "for idx, row in tqdm(grouped_df.iterrows()):\n",
    "    group_id = row['Period']\n",
    "    if group_id not in data_dict:\n",
    "        data_dict[group_id] = defaultdict(list)\n",
    "\n",
    "    for col in grouped_df.columns:\n",
    "            data_dict[group_id][col].append(row[col])\n",
    "    data_dict[group_id]\n",
    "\n",
    "bgl_data_df = pd.DataFrame(data_dict.values())\n",
    "# Add labels to each group\n",
    "bgl_data_df['EventLabels'] = bgl_data_df['Label'].apply(lambda x: [0 if item=='-' else 1 for item in x])\n",
    "bgl_data_df['Label'] = bgl_data_df['Label'].apply(lambda x: 0 if set(x) == set('-') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BGL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default argument for \"using_event_template\"\n",
      "Using default argument for \"batch_size\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-be38ee7521d6d37a\n",
      "Found cached dataset json (/home/dsi/yufli/.cache/huggingface/datasets/json/default-be38ee7521d6d37a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 1/1 [00:00<00:00, 64.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from graph_dataset import BGLDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Test graph dataset\n",
    "root = 'dataset/BGL/seq2seq-node-0.5min-template-bertembed/'\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "# df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    # plm=plm,\n",
    "    # ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    ")\n",
    "bgl_data = BGLDataset(root, hparams=hparams)\n",
    "bgl_loader = DataLoader(bgl_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics (using huggingface dataset)\n",
    "bgl_stats_data = bgl_data.graph_stats\n",
    "print(bgl_stats_data) # |G| = 36169\n",
    "print(bgl_stats_data[0])\n",
    "# # Retrieve normal and anomaly indices\n",
    "# anomaly_ids = [i for i, y in enumerate(bgl_stats_data['label']) if y == 1]\n",
    "# normal_ids = [i for i, y in enumerate(bgl_stats_data['label']) if y == 0]\n",
    "# print(\"#anomly {}, #normal {}\".format(len(anomaly_ids), len(normal_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using template as node attribute input\n",
    "model_path = 'bert-large-uncased' # 'bert-large-uncased', 'facebook/bart-large', 'results/BART_seq2seq/bgl/10-shot-0-10negrate/'\n",
    "plm = AutoModel.from_pretrained(model_path)\n",
    "ptk = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "strategy=0\n",
    "batch_size=46\n",
    "\n",
    "graph_dict = bgl_stats_data[11]\n",
    "ent_tag_dict = {x[1]:x[0] for x in graph_dict['nodes']} # {entity: tag}\n",
    "graph_prompts = []\n",
    "\n",
    "for ent, tag in ent_tag_dict.items():\n",
    "    if tag == 'event':\n",
    "        prompt = ent + ' is an event ID .'\n",
    "    elif tag == 'component':\n",
    "        prompt = ent + ' is a log message component .'\n",
    "    else:\n",
    "        prompt = ent + LABEL2TEMPLATE[tag][strategy]\n",
    "    # print(prompt)\n",
    "    graph_prompts.append(prompt)\n",
    "\n",
    "# print(len(graph_prompts))\n",
    "# Batch handling\n",
    "if len(graph_prompts) > batch_size:\n",
    "    feature = []\n",
    "    num_batch = math.ceil(len(graph_prompts)/batch_size)\n",
    "    for i in range(num_batch):\n",
    "        batch_prompts = graph_prompts[i*batch_size: min(len(graph_prompts), (i+1)*batch_size)]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized_inputs = ptk(\n",
    "            batch_prompts, \n",
    "            max_length=1024,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Pad dynamically \n",
    "        batch = ptk.pad(\n",
    "            tokenized_inputs,\n",
    "            padding=True,\n",
    "            max_length=1024,\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors=\"pt\",\n",
    "        ) # ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "        # Encode\n",
    "        encode_outputs = plm(**batch) # ['last_hidden_state', 'pooler_output']\n",
    "        feature.append(encode_outputs.pooler_output.detach().cpu()) # B X H\n",
    "\n",
    "    feature = torch.cat(feature, dim=0)\n",
    "    print(feature.shape)\n",
    "\n",
    "else:\n",
    "    # Tokenize\n",
    "    tokenized_inputs = ptk(\n",
    "        graph_prompts, \n",
    "        max_length=1024,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Pad dynamically \n",
    "    batch = ptk.pad(\n",
    "        tokenized_inputs,\n",
    "        padding=True,\n",
    "        max_length=1024,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_tensors=\"pt\",\n",
    "    ) # ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "    # Encode\n",
    "    encode_outputs = plm(**batch) # ['last_hidden_state', 'pooler_output']\n",
    "    feature = encode_outputs.pooler_output.detach().cpu() # B X H\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "k = 10\n",
    "# randomly chose k normal and anomaly graphs\n",
    "a_sub_ids = random.sample(range(len(bgl_data)), k=k)\n",
    "for idx in a_sub_ids:\n",
    "    bgl_data._visualize(idx)\n",
    "# n_sub_ids = random.sample(normal_ids, k=k)\n",
    "# for idx in n_sub_ids:\n",
    "#     bgl_data._visualize(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_model import GCNGraphEmbedding, GCNNodeEmbedding\n",
    "\n",
    "m = GCNGraphEmbedding(0.5, 1041, 128, 3)\n",
    "data0 = bgl_data[0]\n",
    "for i, batch in enumerate(bgl_loader):\n",
    "    if i == 0:\n",
    "        batch0 = batch\n",
    "        break\n",
    "outputs = m(x=data0['x'], \n",
    "    edge_index=data0['edge_index'], \n",
    "    batch = torch.LongTensor([0]*data0['x'].shape[0])\n",
    ")\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGL Node Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "\n",
    "root = 'dataset/BGL/regex-node-2min'\n",
    "# Test graph dataset\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained('t5-large')\n",
    "# seq2seq = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'ae-gcnae'}\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    plm=plm,\n",
    "    ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "bgl_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "bgl_node_loader = DataLoader(bgl_node_data, batch_size=64, shuffle=False)\n",
    "node_model = AENodeConv(hparams)\n",
    "node_batch = next(iter(bgl_node_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics (using huggingface dataset)\n",
    "bgl_node_stats_data = bgl_node_data.graph_stats\n",
    "print(bgl_node_stats_data)\n",
    "# Retrieve normal and anomaly indices\n",
    "anomaly_ids = [i for i, y in enumerate(bgl_node_stats_data['label']) if sum(y) > 0]\n",
    "normal_ids = [i for i, y in enumerate(bgl_node_stats_data['label']) if sum(y) == 0]\n",
    "print(\"#anomly {}, #normal {}\".format(len(anomaly_ids), len(normal_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "k = 10\n",
    "# randomly chose k normal and anomaly graphs\n",
    "a_sub_ids = random.sample(anomaly_ids, k=k)\n",
    "for idx in a_sub_ids:\n",
    "    bgl_node_data._visualize(idx)\n",
    "n_sub_ids = random.sample(normal_ids, k=k)\n",
    "for idx in n_sub_ids:\n",
    "    bgl_node_data._visualize(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly case study (BGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4713493, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Label</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Date</th>\n",
       "      <th>Node</th>\n",
       "      <th>Time</th>\n",
       "      <th>NodeRepeat</th>\n",
       "      <th>Type</th>\n",
       "      <th>Component</th>\n",
       "      <th>Level</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>ParameterList</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.50.363779</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>3aa50e45</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-06-03 18:42:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.50.527847</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>3aa50e45</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-06-03 18:42:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.50.675872</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>3aa50e45</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-06-03 18:42:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.50.823719</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>3aa50e45</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-06-03 18:42:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03-15.42.50.982731</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>3aa50e45</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-06-03 18:42:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Label   Timestamp        Date                 Node  \\\n",
       "0       1     -  1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "1       2     -  1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "2       3     -  1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "3       4     -  1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "4       5     -  1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "\n",
       "                         Time           NodeRepeat Type Component Level  \\\n",
       "0  2005-06-03-15.42.50.363779  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "1  2005-06-03-15.42.50.527847  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "2  2005-06-03-15.42.50.675872  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "3  2005-06-03-15.42.50.823719  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "4  2005-06-03-15.42.50.982731  R02-M1-N0-C:J12-U11  RAS    KERNEL  INFO   \n",
       "\n",
       "                                    Content   EventId  \\\n",
       "0  instruction cache parity error corrected  3aa50e45   \n",
       "1  instruction cache parity error corrected  3aa50e45   \n",
       "2  instruction cache parity error corrected  3aa50e45   \n",
       "3  instruction cache parity error corrected  3aa50e45   \n",
       "4  instruction cache parity error corrected  3aa50e45   \n",
       "\n",
       "                              EventTemplate ParameterList  Tag  \\\n",
       "0  instruction cache parity error corrected            []    0   \n",
       "1  instruction cache parity error corrected            []    0   \n",
       "2  instruction cache parity error corrected            []    0   \n",
       "3  instruction cache parity error corrected            []    0   \n",
       "4  instruction cache parity error corrected            []    0   \n",
       "\n",
       "             Datetime  \n",
       "0 2005-06-03 18:42:50  \n",
       "1 2005-06-03 18:42:50  \n",
       "2 2005-06-03 18:42:50  \n",
       "3 2005-06-03 18:42:50  \n",
       "4 2005-06-03 18:42:50  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file = 'dataset/BGL/BGL.log_structured.csv' # the structured log file\n",
    "bgl_df = pd.read_csv(log_file, na_filter=False, memory_map=True)\n",
    "bgl_df['Tag'] = bgl_df['Label'].apply(lambda x: 0 if x == '-' else 1)\n",
    "bgl_df['Datetime'] = bgl_df['Timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "print(bgl_df.shape)\n",
    "bgl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import splitbyinterval, get_train_test_data\n",
    "\n",
    "# sub_bgl_df = bgl_df.loc[bgl_df.LineId <= 100000]\n",
    "interval = '0.5min'\n",
    "grouped_df = splitbyinterval(bgl_df, interval)\n",
    "data_dict = OrderedDict()\n",
    "for idx, row in tqdm(grouped_df.iterrows()):\n",
    "    group_id = row['Period']\n",
    "    if group_id not in data_dict:\n",
    "        data_dict[group_id] = defaultdict(list)\n",
    "\n",
    "    for col in grouped_df.columns:\n",
    "        data_dict[group_id][col].append(row[col])\n",
    "    data_dict[group_id]\n",
    "\n",
    "data_df = pd.DataFrame(data_dict.values())\n",
    "# Add labels to each group\n",
    "data_df['EventLabels'] = data_df['Label'].apply(lambda x: [0 if item=='-' else 1 for item in x])\n",
    "data_df['Label'] = data_df['Label'].apply(lambda x: 0 if set(x) == set('-') else 1)\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1848, 4)\n",
      "(1848, 4) (500, 4) (270, 4) (1078, 4)\n"
     ]
    }
   ],
   "source": [
    "bgl_event_df = bgl_df.groupby('EventId').head(1)[['Content', 'EventTemplate', 'ParameterList']]\n",
    "bgl_event_df['ner_tags'] = [[] for _ in range(len(bgl_event_df))]\n",
    "print(bgl_event_df.shape)\n",
    "bgl_event_df.to_csv('dataset/BGL/BGL.log_structured_event_1.csv')\n",
    "train_event_df = bgl_event_df.sample(n=500, random_state=seed)\n",
    "train_event_df.to_csv('dataset/BGL/BGL.log_structured_event_train.csv', index=False)\n",
    "remain_event_df = bgl_event_df[~bgl_event_df.index.isin(train_event_df.index)]\n",
    "test_event_df = remain_event_df.sample(frac=0.8, random_state=seed)\n",
    "val_event_df = remain_event_df[~remain_event_df.index.isin(test_event_df.index)]\n",
    "val_event_df.to_csv('dataset/BGL/BGL.log_structured_event_val.csv', index=False)\n",
    "test_event_df.to_csv('dataset/BGL/BGL.log_structured_event_test.csv', index=False)\n",
    "print(bgl_event_df.shape, train_event_df.shape, val_event_df.shape, test_event_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "number of named entities does not match tags!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24009/3068349857.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24009/3068349857.py\u001b[0m in \u001b[0;36mprocess_csv\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameterList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of named entities does not match tags!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: number of named entities does not match tags!"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "t_file = 'dataset/BGL/BGL.log_structured_event_train.csv'\n",
    "v_file = 'dataset/BGL/BGL.log_structured_event_val.csv'\n",
    "\n",
    "def process_csv(file):\n",
    "    t_df = pd.read_csv(file)\n",
    "    # Parse string of list\n",
    "    t_df.ParameterList = t_df.ParameterList.apply(literal_eval)\n",
    "    t_df.ner_tags = t_df.ner_tags.apply(literal_eval)\n",
    "    # Get rid of unnamed columns\n",
    "    if 'Unnamed: 0' in t_df.columns:\n",
    "        t_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    # Test correspondence\n",
    "    for i, row in t_df.iterrows():\n",
    "        if len(row.ParameterList) != len(row.ner_tags):\n",
    "            raise ValueError('number of named entities does not match tags!')\n",
    "    return t_df\n",
    "\n",
    "df1 = process_csv(t_file)\n",
    "df2 = process_csv(v_file)\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = process_csv(t_file)\n",
    "df.rename(columns={\n",
    "    \"Content\": \"logex:example\", \n",
    "    \"EventTemplate\": \"logex:pattern\", \n",
    "    \"ParameterList\": \"logex:hasParameterList\", \n",
    "    \"ner_tags\": \"logex:hasNERtag\",\n",
    "}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ENTITY_COLUMN_NAME, TAG_COLUMN_NAME, LOG_COLUMN_NAME\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "data = Dataset.from_pandas(df)\n",
    "\n",
    "# Save dataset\n",
    "out_f = '/nfs/intern_data/yufli/dataset/BGL/dataset.json'\n",
    "data.to_json(out_f)\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset('json', data_files=out_f)['train']\n",
    "\n",
    "# Get tag-entity statistics\n",
    "entity_set = defaultdict(set)\n",
    "entity_count = defaultdict(list)\n",
    "for i, instance in enumerate(data):\n",
    "    for ent, tag in zip(instance[ENTITY_COLUMN_NAME], instance[TAG_COLUMN_NAME]):\n",
    "            entity_set[tag].add(ent)\n",
    "            entity_count[tag].append(i)\n",
    "\n",
    "entity_occ = sum(len(ids) for ids in entity_count.values())\n",
    "print(\"Total #entities: {}, average #entities per log: {:.3f}\".format(entity_occ, entity_occ/len(data)))\n",
    "print(\"Entity distribution ({}): {}\".format(len(entity_count), {k:len(v) for k,v in entity_count.items()}))\n",
    "print('log: \"%s\",'%data[LOG_COLUMN_NAME][0], \n",
    "        'entities: %s,'%data[ENTITY_COLUMN_NAME][0], \n",
    "        'tags: %s.'%data[TAG_COLUMN_NAME][0])\n",
    "for tag, entities in entity_set.items():\n",
    "    print(\"\\t{} ({}): {}\".format(tag, len(entities), entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train(10-shot & 5-shot)/val/test data for NER training\n",
    "n_shots = 10\n",
    "n_shot_ids = []\n",
    "ten_shot_ids = []\n",
    "random.seed(seed)\n",
    "for tag in entity_count:\n",
    "    tag_ids = random.choices(entity_count[tag], k=10) \n",
    "    ten_shot_ids.extend(tag_ids) # 10-shot\n",
    "    n_shot_ids.extend(tag_ids[:n_shots])\n",
    "\n",
    "n_shot_data = data.select(n_shot_ids).shuffle(seed=seed) # n-shot\n",
    "remain_ids = list(set(range(len(data))) - set(ten_shot_ids)) \n",
    "val_ids = random.sample(remain_ids, int(len(remain_ids)*0.3))\n",
    "val_data = data.select(val_ids)\n",
    "test_ids = list(set(remain_ids) - set(val_ids))\n",
    "test_data = data.select(test_ids)\n",
    "print(n_shot_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_anomaly_rate_dict = defaultdict(float)\n",
    "\n",
    "for eventid, ids in bgl_df.groupby('EventId').groups.items():\n",
    "    subgroup = bgl_df.loc[ids]\n",
    "    event_anomaly_rate_dict[eventid] = sum(subgroup.Tag)/len(ids)\n",
    "\n",
    "event_anomaly_rate = sorted(event_anomaly_rate_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(len(event_anomaly_rate))\n",
    "print(event_anomaly_rate[:57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "all_labels = np.array([0 if sum(x) == 0 else 1 for x in bgl_node_data.graph_stats['label']])\n",
    "anomaly_size = sum(all_labels)\n",
    "normal_size = len(all_labels) - anomaly_size\n",
    "\n",
    "n_train = floor(normal_size*0.8) \n",
    "val_size = ceil(n_train*0.2)\n",
    "train_size = floor(n_train*0.8)\n",
    "test_size = len(bgl_node_data) - train_size - val_size\n",
    "test_anomaly_rate = anomaly_size/test_size\n",
    "\n",
    "train_graph_data = bgl_node_data[:train_size]\n",
    "val_graph_data = bgl_node_data[train_size:train_size + val_size]\n",
    "test_graph_data = bgl_node_data[train_size + val_size:]\n",
    "\n",
    "print(len(train_graph_data), len(val_graph_data), len(test_graph_data))\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_graph_data, \n",
    "    batch_size=256, \n",
    "    shuffle=False, \n",
    ")\n",
    "\n",
    "event_id = EMBED_SIZE + tag2id['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = []\n",
    "\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    is_event_nodes = batch['x'][:, event_id] == 1\n",
    "    labels = batch['y'][is_event_nodes]\n",
    "    test_gt.append(labels)\n",
    "\n",
    "test_gt = torch.cat(test_gt, dim=0).numpy() # N (only for event predictions)\n",
    "print(len(test_gt), sum(test_gt), sum(test_gt)/len(test_gt))\n",
    "test_node_stats = bgl_node_data.graph_stats.select(range(train_size + val_size, len(bgl_node_data)))\n",
    "test_node_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.read_csv('results/bgl/regex-node/GCN-mlp-2min-new/predictions.csv', na_filter=False, memory_map=True)\n",
    "test_pred.drop(test_pred.columns[test_pred.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "\n",
    "event_count = []\n",
    "idx = 0\n",
    "\n",
    "for graph_id, graph_dict in tqdm(enumerate(test_node_stats)):\n",
    "    ent_tag_dict = {x[1]:x[0] for x in graph_dict['nodes']} # {entity: tag}\n",
    "    for ent, tag in ent_tag_dict.items():\n",
    "        if tag == 'event':\n",
    "            value = {'eventId': ent, 'graphId': train_size + val_size + graph_id}\n",
    "            row = test_pred.iloc[idx]\n",
    "            for col in test_pred.columns:\n",
    "                value[col] = row[col]\n",
    "            event_count.append(value)\n",
    "            idx += 1\n",
    "\n",
    "event_pred_df = pd.DataFrame(event_count)\n",
    "event_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stats = {}\n",
    "TP_graph_set, FN_graph_set, FP_graph_set, TN_graph_set = set(), set(), set(), set()\n",
    "\n",
    "for idx, row in tqdm(event_pred_df.iterrows()):\n",
    "    y = 'anomaly' if row.GT == 1 else 'normal'\n",
    "    if row.eventId not in event_stats:\n",
    "        event_stats[row.eventId] = {'TP': [], 'FP': [], 'TN': [], 'FN': []}\n",
    "\n",
    "    if row.GT == 1:\n",
    "        if row['top80%'] == 1:\n",
    "            event_stats[row.eventId]['TP'].append(row.graphId)\n",
    "            if row.graphId not in TP_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_TP.html')\n",
    "                TP_graph_set.add(row.graphId)\n",
    "        else:\n",
    "            event_stats[row.eventId]['FN'].append(row.graphId)\n",
    "            if row.graphId not in FN_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_FN.html')\n",
    "                FN_graph_set.add(row.graphId)\n",
    "    else:\n",
    "        if row['top80%'] == 1:\n",
    "            event_stats[row.eventId]['FP'].append(row.graphId)\n",
    "            if row.graphId not in FP_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_TP.html')\n",
    "                FP_graph_set.add(row.graphId)\n",
    "        else:\n",
    "            event_stats[row.eventId]['TN'].append(row.graphId)\n",
    "            if row.graphId not in TN_graph_set:\n",
    "                # bgl_node_data._visualize(row.graphId, name=f'graph{row.graphId}_{row.eventId}_FN.html')\n",
    "                TN_graph_set.add(row.graphId)\n",
    "\n",
    "for event, stats in event_stats.items():\n",
    "    stats['Precision'] = len(stats['TP'])/(len(stats['TP'])+len(stats['FP'])) if len(stats['TP'])+len(stats['FP']) else 0\n",
    "    stats['Recall'] = len(stats['TP'])/(len(stats['TP'])+len(stats['FN'])) if len(stats['TP'])+len(stats['FN']) else 0\n",
    "    stats['F1'] = 2*stats['Precision']*stats['Recall']/(stats['Precision']+stats['Recall']) if stats['Precision']+stats['Recall'] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_events = set([x[0] for x in event_anomaly_rate[:57]])\n",
    "anomaly_event_stats = []\n",
    "for event, stats in event_stats.items():\n",
    "    if event in anomaly_events:\n",
    "        stats['anomaly_rate'] = event_anomaly_rate_dict[event]\n",
    "        stats['eventId'] = event\n",
    "        anomaly_event_stats.append(stats)\n",
    "\n",
    "anomaly_event_df = pd.DataFrame(anomaly_event_stats)\n",
    "cols = anomaly_event_df.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "anomaly_event_df = anomaly_event_df[cols]\n",
    "anomaly_event_df = anomaly_event_df.sort_values(by=['F1', 'Precision'], ascending=False, ignore_index=True)\n",
    "anomaly_event_df.to_csv('results/bgl/regex-node/GCN-mlp-2min-new/event-results-analysis.csv', index=False)\n",
    "anomaly_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(anomaly_event_df.iterrows()):\n",
    "    if len(row.TP) and len(row.FN):\n",
    "        for graphId in row.TP:\n",
    "            visualize(\n",
    "                graph_stats=bgl_node_data.graph_stats, \n",
    "                root=bgl_node_data.root, \n",
    "                out_dir=os.path.join(bgl_node_data.root, 'graph', row.eventId), \n",
    "                idx=graphId, \n",
    "                name=f'graph_{graphId}_TP.html',\n",
    "            )\n",
    "        for graphId in row.FN:\n",
    "            visualize(\n",
    "                graph_stats=bgl_node_data.graph_stats, \n",
    "                root=bgl_node_data.root, \n",
    "                out_dir=os.path.join(bgl_node_data.root, 'graph', row.eventId), \n",
    "                idx=graphId, \n",
    "                name=f'graph_{graphId}_FN.html',\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = EMBED_SIZE + tag2id['event']\n",
    "is_event_nodes = batch['x'][:, event_id] == 1\n",
    "event_x= batch['x'][is_event_nodes]\n",
    "preds = node_model(\n",
    "    x=batch['x'], \n",
    "    edge_index=batch['edge_index'], \n",
    "    batch=batch['batch'],\n",
    ")\n",
    "event_preds = preds[is_event_nodes]\n",
    "event_labels = batch['y'][is_event_nodes]\n",
    "print(event_x.shape, event_preds.shape, event_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch end\n",
    "event2preds = defaultdict(list)\n",
    "event_x_list = event_x.tolist()\n",
    "\n",
    "for x, preds in zip(event_x_list, event_preds):\n",
    "    event2preds[tuple(x)].append(preds)\n",
    "    \n",
    "print(len(event2preds))\n",
    "print([len(v) for k,v in event2preds.items()])\n",
    "print(sum(len(v) == 1 for k,v in event2preds.items()))\n",
    "\n",
    "# Average embeddings for each event\n",
    "event2avg = {k: torch.stack(preds, dim=0).mean(dim=0) for k, preds in event2preds.items()}\n",
    "print(len(event2avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epoch\n",
    "train_dists = defaultdict(list)\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "targets = []\n",
    "for x in event_x_list:\n",
    "    targets.append(event2avg[tuple(x)])\n",
    "\n",
    "targets = torch.stack(targets, dim=0)\n",
    "print(event_preds.size(), targets.size())\n",
    "individual_loss = mse_loss(event_preds, targets).sum(dim=-1) # B\n",
    "\n",
    "for i, x in enumerate(event_x_list):\n",
    "    train_dists[tuple(x)].append(individual_loss[i].detach().item())\n",
    "    \n",
    "print(len(train_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AE Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from argparse import Namespace\n",
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "from SetBart import BartForConditionalGeneration\n",
    "\n",
    "root = '/nfs/intern_data/yufli/dataset/BGL/regex-node-halfmin-template'\n",
    "\n",
    "# Test graph dataset\n",
    "plm = AutoModel.from_pretrained('bert-large-uncased')\n",
    "ptk = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# seq2seq = AutoModelForSeq2SeqLM.from_pretrained('t5-large')\n",
    "config = AutoConfig.from_pretrained('facebook/bart-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')\n",
    "seq2seq2 = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'ae-gcnae'}\n",
    "\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    plm=plm,\n",
    "    ptk=ptk,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "bgl_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "bgl_node_loader = DataLoader(bgl_node_data, batch_size=8, shuffle=False)\n",
    "node_model = AENodeConv(hparams)\n",
    "\n",
    "# Modify bart VOCAB\n",
    "seq2seq.resize_token_embeddings(bgl_node_data.num_nodes)\n",
    "seq2seq2.resize_token_embeddings(bgl_node_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = bgl_node_data[0]\n",
    "stats0 = bgl_node_data.graph_stats[0]\n",
    "ent_tag_dict0 = {x[1]:x[0] for x in stats0['nodes']} # {entity: tag}\n",
    "print(x0)\n",
    "print(stats0)\n",
    "print(ent_tag_dict0)\n",
    "print(list(ent_tag_dict0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "merged_data = Data(x=x0.x, edge_index=x0.edge_index, y=x0.y, ids=x0.ids, node_strs=list(ent_tag_dict0))\n",
    "print(merged_data)\n",
    "print(merged_data.node_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(bgl_node_loader):\n",
    "    if i == 0:\n",
    "        batch0 = batch\n",
    "    elif i == 1:\n",
    "        batch1 = batch\n",
    "    elif i == 2:\n",
    "        batch2 = batch\n",
    "    elif i == 3:\n",
    "        batch3 = batch\n",
    "    elif i == 4:\n",
    "        batch4 = batch\n",
    "    elif i == 5:\n",
    "        batch5 = batch\n",
    "    elif i == 6:\n",
    "        batch6 = batch\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "out_channels = 128\n",
    "num_layers = 4\n",
    "embed_dim = 768\n",
    "# split the number of layers for the encoder and decoders\n",
    "decoder_layers = int(num_layers / 2)\n",
    "encoder_layers = num_layers - decoder_layers\n",
    "num_neigh = -1\n",
    "alpha = 0.5\n",
    "\n",
    "gcn_encoder = GCN(in_channels=batch0.x.shape[1],\n",
    "            hidden_channels=out_channels,\n",
    "            out_channels=embed_dim,\n",
    "            num_layers=encoder_layers,\n",
    "            dropout=0.3,\n",
    "            act=F.relu)\n",
    "attr_decoder = GCN(in_channels=embed_dim,\n",
    "            hidden_channels=out_channels,\n",
    "            out_channels=batch0.x.shape[1],\n",
    "            num_layers=decoder_layers,\n",
    "            dropout=0.3,\n",
    "            act=F.relu)\n",
    "struct_decoder = GCN(in_channels=embed_dim,\n",
    "            hidden_channels=out_channels,\n",
    "            out_channels=batch0.x.shape[1],\n",
    "            num_layers=decoder_layers - 1,\n",
    "            dropout=0.3,\n",
    "            act=F.relu)\n",
    "\n",
    "def get_half_index(batch, batch_size=8):\n",
    "    half_idx = (batch >= int(batch_size/2)).nonzero()[0].item()\n",
    "    return half_idx\n",
    "\n",
    "def get_preds_from_batch(node_batch, gcn_model):\n",
    "    node_idx = torch.arange(node_batch.x.shape[0])\n",
    "    s = to_dense_adj(node_batch.edge_index)[0] # |V| X |V|\n",
    "    x_ = gcn_model(x=node_batch.x, edge_index=node_batch.edge_index) # |V| X 1024\n",
    "    # Split by half number of graphs in this batch\n",
    "    half_idx = get_half_index(node_batch.batch)\n",
    "    former_batch = batch0.batch[:half_idx]\n",
    "    former_x_ = x_[:half_idx]\n",
    "    former_ids = node_batch.ids[:half_idx]\n",
    "\n",
    "    latter_batch = batch0.batch[half_idx:]\n",
    "    latter_x_ = x_[half_idx:]\n",
    "    latter_ids = node_batch.ids[half_idx:]\n",
    "    \n",
    "    return x_.unsqueeze(0), former_ids.unsqueeze(0), former_x_.unsqueeze(0), \\\n",
    "        latter_ids.unsqueeze(0), latter_x_.unsqueeze(0)\n",
    "\n",
    "def to_sparse_batch(x, mask):\n",
    "    h = x.shape[-1]\n",
    "    if x.dim() == 3:\n",
    "        mask_ = mask.unsqueeze(-1).expand(x.size())\n",
    "        return torch.masked_select(x, mask_).reshape(-1, h) # |V| X 1024\n",
    "    elif x.dim() == 2:\n",
    "        return torch.masked_select(x, mask) # |V|\n",
    "    \n",
    "\n",
    "x0_, x0_former_ids, x0_former_embed, x0_latter_ids, x0_latter_embed = get_preds_from_batch(batch0, gcn_encoder)\n",
    "x1_, x1_former_ids, x1_former_embed, x1_latter_ids, x1_latter_embed = get_preds_from_batch(batch1, gcn_encoder)\n",
    "x2_, x2_former_ids, x2_former_embed, x2_latter_ids, x2_latter_embed = get_preds_from_batch(batch2, gcn_encoder)\n",
    "x3_, x3_former_ids, x3_former_embed, x3_latter_ids, x3_latter_embed = get_preds_from_batch(batch3, gcn_encoder)\n",
    "x4_, x4_former_ids, x4_former_embed, x4_latter_ids, x4_latter_embed = get_preds_from_batch(batch4, gcn_encoder)\n",
    "x5_, x5_former_ids, x5_former_embed, x5_latter_ids, x5_latter_embed = get_preds_from_batch(batch5, gcn_encoder)\n",
    "x6_, x6_former_ids, x6_former_embed, x6_latter_ids, x6_latter_embed = get_preds_from_batch(batch6, gcn_encoder)\n",
    "print([item.shape for item in [x0_, x0_former_ids, x0_former_embed, x0_latter_ids, x0_latter_embed]])\n",
    "print([item.shape for item in [x1_, x1_former_ids, x1_former_embed, x1_latter_ids, x1_latter_embed]])\n",
    "print([item.shape for item in [x2_, x2_former_ids, x2_former_embed, x2_latter_ids, x2_latter_embed]])\n",
    "print([item.shape for item in [x3_, x3_former_ids, x3_former_embed, x3_latter_ids, x3_latter_embed]])\n",
    "print([item.shape for item in [x4_, x4_former_ids, x4_former_embed, x4_latter_ids, x4_latter_embed]])\n",
    "print([item.shape for item in [x5_, x5_former_ids, x5_former_embed, x5_latter_ids, x5_latter_embed]])\n",
    "print([item.shape for item in [x6_, x6_former_ids, x6_former_embed, x6_latter_ids, x6_latter_embed]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "torch.manual_seed(8)\n",
    "\n",
    "# handle node and edge index\n",
    "G = batch5\n",
    "max_length = 512\n",
    "perm = torch.randperm(G.num_graphs)\n",
    "print(perm)\n",
    "print(G)\n",
    "for graph_id in perm:\n",
    "    print(G.get_example(graph_id))\n",
    "# print(G.get_example(0))\n",
    "# print(G.get_example(0).edge_index)\n",
    "# print(G.get_example(2))\n",
    "# print(G.get_example(2).edge_index)\n",
    "# print(G.from_data_list([G.get_example(0), G.get_example(2)]))\n",
    "# print(G.from_data_list([G.get_example(0), G.get_example(2)]).edge_index)\n",
    "\n",
    "accum_nodes = 0\n",
    "data_list = []\n",
    "\n",
    "for graph_id in perm:\n",
    "    data = G.get_example(graph_id)\n",
    "    if accum_nodes + data.num_nodes <= max_length:\n",
    "        accum_nodes += data.num_nodes\n",
    "        data_list.append(data)\n",
    "\n",
    "sub_G = G.from_data_list(data_list)\n",
    "\n",
    "print('Subgraph', sub_G)\n",
    "# perm = torch.randperm(G.num_nodes)\n",
    "# print(G.x)\n",
    "# print(G.edge_index)\n",
    "# idx = perm[:max_length]\n",
    "# print(idx)\n",
    "# sub_G = G.subgraph(idx)\n",
    "print(sub_G.x)\n",
    "print(sub_G.edge_index)\n",
    "# print(sub_G.edge_index.max(), sub_G.edge_index.min())\n",
    "if not sub_G.edge_index.shape[-1]:\n",
    "    # Empty edge index\n",
    "    print(\"Empty edge index !!!\")\n",
    "    s0 = torch.zeros((sub_G.num_nodes, sub_G.num_nodes))\n",
    "else:\n",
    "    s0 = to_dense_adj(sub_G.edge_index, max_num_nodes=len(idx))[0]\n",
    "print(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seq2seq loss\n",
    "# encoder_output2 = seq2seq.encoder(inputs_embeds=x_2.unsqueeze(0)) # for t5\n",
    "max_length = config.max_position_embeddings\n",
    "\n",
    "output = seq2seq(\n",
    "    inputs_embeds=x5_former_embed[:, :max_length, :], \n",
    "    labels=x5_latter_ids[:, :max_length],\n",
    "    decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for bart\n",
    "\n",
    "print(output.keys()) # 'loss', 'logits', 'encoder_last_hidden_state'\n",
    "print(output.loss) # float\n",
    "print(output.logits.shape) # B X T (output seq) X vocab_size\n",
    "print(output.logits)\n",
    "print(output.encoder_last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seq2seq loss\n",
    "# encoder_output2 = seq2seq.encoder(inputs_embeds=x_2.unsqueeze(0)) # for t5\n",
    "max_length = config.max_position_embeddings\n",
    "\n",
    "output2 = seq2seq2(\n",
    "    inputs_embeds=x5_former_embed[:, :max_length, :], \n",
    "    labels=x5_latter_ids[:, :max_length],\n",
    "    decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for bart\n",
    "\n",
    "print(output2.keys()) # 'loss', 'logits', 'encoder_last_hidden_state'\n",
    "print(output2.loss) # float\n",
    "print(output2.logits.shape) # B X T (output seq) X vocab_size\n",
    "print(output2.logits)\n",
    "print(output2.encoder_last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_config = AutoConfig.from_pretrained('xlnet-large-cased')\n",
    "xnlet_base = AutoModel.from_pretrained('xlnet-base-cased')\n",
    "xnlet_large = AutoModel.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "# Modify bart VOCAB\n",
    "xnlet_base.resize_token_embeddings(bgl_node_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xnlet_model decoder\n",
    "# from transformers.models.xlnet.modeling_xlnet\n",
    "\n",
    "decoder_output_final = xnlet_base(\n",
    "    inputs_embeds=x5_, \n",
    "    # decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for xlnet\n",
    "\n",
    "print(decoder_output_final.keys()) # 'last_hidden_state', 'mems'\n",
    "print(decoder_output_final.last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_config = AutoConfig.from_pretrained('gpt2')\n",
    "gpt2_model = AutoModel.from_pretrained('gpt2')\n",
    "\n",
    "# Modify bart VOCAB\n",
    "gpt2_model.resize_token_embeddings(bgl_node_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpe = nn.Embedding(gpt2_config.max_position_embeddings, gpt2_config.hidden_size)\n",
    "position_ids = torch.arange(0, batch5.num_graphs, dtype=torch.long)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, batch5.num_graphs)\n",
    "print(position_ids)\n",
    "graph_embed = wpe(position_ids).squeeze(0)\n",
    "print(graph_embed.shape)\n",
    "\n",
    "node_embed = torch.stack([graph_embed[graphid] for graphid in batch5.batch], dim=0) # |V| X 1024\n",
    "print(node_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model.resize_token_embeddings(131313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '102.10.67.322 is an IP address.'\n",
    "ids1 = bert_tokenizer.encode(text, add_special_tokens=False)\n",
    "ids2 = bert_tokenizer(text, add_special_tokens=False)\n",
    "\n",
    "print(ids1)\n",
    "print(ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.embeddings.word_embeddings.weight[ids1].sum(dim=0).detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.bert.modeling_bert\n",
    "\n",
    "encoder_output_bert = bert_model.encoder(\n",
    "    x5_, \n",
    "    # decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for xlnet\n",
    "\n",
    "print(decoder_output_final.keys()) # 'last_hidden_state', 'mems'\n",
    "print(decoder_output_final.last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.gpt2.modeling_gpt2\n",
    "\n",
    "max_length = gpt2_config.max_position_embeddings\n",
    "\n",
    "decoder_output_gpt2 = gpt2_model(\n",
    "    inputs_embeds=x5_[:, :max_length, :], \n",
    "    # decoder_inputs_embeds=x5_latter_embed[:, :max_length, :],\n",
    ") # for gpt2\n",
    "\n",
    "print(decoder_output_gpt2.keys()) # 'last_hidden_state', 'past_key_values'\n",
    "print(decoder_output_gpt2.last_hidden_state.shape) # B X T (input seq) X H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequence\n",
    "encoder_output2 = seq2seq2.model.encoder(\n",
    "    inputs_embeds=x5_[:, :max_length, :],\n",
    ")\n",
    "print(encoder_output2.keys())\n",
    "print(encoder_output2.last_hidden_state.shape)\n",
    "print(encoder_output2.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequence (BERT)\n",
    "encoder_output3 = plm.encoder(\n",
    "    hidden_states=x5_[:, :max_length, :], \n",
    ")\n",
    "print(encoder_output3.keys())\n",
    "print(encoder_output3.last_hidden_state.shape)\n",
    "print(encoder_output3.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN decode (reconstruction)\n",
    "pair_max, _ = batch5.edge_index.max(dim=0)\n",
    "cutted_edge_index = batch5.edge_index.T[pair_max < max_length].T\n",
    "\n",
    "hidden = encoder_output.last_hidden_state.squeeze(0) # |V_cute| X 1024\n",
    "x_recover = attr_decoder(x=hidden.detach().cpu(), edge_index=cutted_edge_index)\n",
    "\n",
    "# Decode adjacency matrix\n",
    "h_ = struct_decoder(hidden.detach().cpu(), cutted_edge_index)\n",
    "s_ = h_ @ h_.T\n",
    "\n",
    "print(x_recover.shape, s_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistic graph number of nodes\n",
    "from tqdm import tqdm\n",
    "\n",
    "node_nums = []\n",
    "for i, ins in tqdm(enumerate(bgl_node_data.graph_stats)):\n",
    "    node_set = set([x[1] for x in ins['nodes']])\n",
    "    node_nums.append(len(node_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "node_nums = np.array(node_nums)\n",
    "hundred_rate = sum(node_nums > 128)/len(node_nums)\n",
    "print('{:4f}% of graphs with more than 128 nodes'.format(hundred_rate*100))\n",
    "thousand_rate = sum(node_nums > 1024)/len(node_nums)\n",
    "print('{:4f}% of graphs with more than 1024 nodes'.format(thousand_rate*100))\n",
    "two_thousand_rate = sum(node_nums > 2048)/len(node_nums)\n",
    "print('{:4f}% of graphs with more than 2048 nodes'.format(two_thousand_rate*100))\n",
    "\n",
    "node_dist = Counter(node_nums)\n",
    "# print(node_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZE_PATTERN = ' |(=)|(:) |([()])|(,) |([\\[\\]])|([{}])|([<>])|(\\.) |(\\.$)'\n",
    "# s = \"Power Good signal deactivated: R73-M1-N5. A service action may be required.\"\n",
    "s = \"monitor caught java.lang.IllegalStateException: while executing I2C Operation caught java.net.SocketException: Broken pipe and is stopping\"\n",
    "words = list(filter(None, re.split(TOKENIZE_PATTERN, s)))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIT Node Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from argparse import Namespace\n",
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "\n",
    "root = '/nfs/intern_data/yufli/dataset/AIT/seq2seq-node-0.5min-template'\n",
    "\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'ae-gcnae'}\n",
    "\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "ait_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "ait_node_loader = DataLoader(ait_node_data, batch_size=8, shuffle=False)\n",
    "b0 = next(iter(ait_node_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Transformer forward\n",
    "\n",
    "# Graph encode\n",
    "e_ = gcn_encoder(b0.x, b0.edge_index) # |V| X E\n",
    "\n",
    "# Add position embedding (graph-level)\n",
    "wpe = nn.Embedding(gpt2_config.max_position_embeddings, gpt2_config.hidden_size)\n",
    "position_ids = torch.arange(0, b0.num_graphs, dtype=torch.long)\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, b0.num_graphs) # |G|\n",
    "graph_embed = wpe(position_ids).squeeze(0) # |G| X E\n",
    "node_embed = torch.stack([graph_embed[graphid] for graphid in b0.batch], dim=0) # |V| X E\n",
    "e_ = e_ + node_embed # |V| X E\n",
    "\n",
    "# GPT2 LM\n",
    "outputs = gpt2_lm_model(\n",
    "    inputs_embeds=e_.unsqueeze(0),\n",
    "    labels=b0.ids.unsqueeze(0),\n",
    ")\n",
    "print(outputs.keys()) # ['loss', 'logits', 'past_key_values', 'hidden_states']\n",
    "print(outputs.loss)\n",
    "last_hidden_state = outputs.hidden_states[-1].squeeze(0) # |V| X E\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/BART_seq2seq/ait/10-shot-0'\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = 0\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# seq2seq = seq2seq\n",
    "\n",
    "# def get_attribute(examples):\n",
    "\n",
    "#     all_graph_ids = []\n",
    "#     for i, node_pair_list in enumerate(examples['nodes']):\n",
    "        \n",
    "#         ent_tag_dict = {x[1]:x[0] for x in node_pair_list} # {entity: tag}\n",
    "#         # print(ent_tag_dict)\n",
    "\n",
    "#         # For each graph, gets template list\n",
    "#         graph_templates = []\n",
    "#         for ent, tag in ent_tag_dict.items():\n",
    "#             # print('ent {} tag {}'.format(ent, tag))\n",
    "#             # First get template\n",
    "#             if tag == 'event':\n",
    "#                 template = ent + ' is an event ID .'\n",
    "#             elif tag == 'component':\n",
    "#                 template = ent + ' is a log message component .'\n",
    "#             else:\n",
    "#                 template = ent + LABEL2TEMPLATE[tag][strategy]\n",
    "            \n",
    "#             graph_templates.append(template)\n",
    "\n",
    "#         # Tokenize and encode each template in a graph\n",
    "#         tokenized_inputs = tokenizer(\n",
    "#             graph_templates,\n",
    "#             padding=False,\n",
    "#             truncation=True,\n",
    "#             max_length=1024,\n",
    "#         )\n",
    "\n",
    "#         # Pad dynamically \n",
    "#         batch = tokenizer.pad(\n",
    "#             tokenized_inputs,\n",
    "#             padding=True,\n",
    "#             max_length=1024,\n",
    "#             pad_to_multiple_of=8,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "\n",
    "#         # Get Encoder outputs\n",
    "#         encoder_outputs = seq2seq.model.encoder(**batch)\n",
    "\n",
    "#         all_graph_ids.append(encoder_outputs.last_hidden_state)\n",
    "\n",
    "#         # print(tokenized_inputs)\n",
    "    \n",
    "#     examples['embeddings'] = all_graph_ids\n",
    "#     return examples\n",
    "\n",
    "\n",
    "# subdata = ait_node_data.graph_stats.select(range(100)).map(\n",
    "#     get_attribute,\n",
    "#     batched=True,\n",
    "#     num_proc=None,\n",
    "#     load_from_cache_file=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from argparse import Namespace\n",
    "from graph_dataset import BGLNodeDataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from graph_model import NodeConv, GCN, AENodeConv\n",
    "\n",
    "root = '/nfs/intern_data/yufli/dataset/BGL/seq2seq-edge-0.5min-template-bertembed'\n",
    "\n",
    "# Define tag_to_id dict\n",
    "tag2id = {ent:i for i, ent in enumerate(LABEL2TEMPLATE.keys())}\n",
    "tag2id['event'] = len(tag2id)\n",
    "tag2id['component'] = len(tag2id)\n",
    "df = pd.DataFrame([])\n",
    "model_kwargs = {'model_type': 'dynamic'}\n",
    "\n",
    "# Define model args\n",
    "in_channels = EMBED_SIZE + len(tag2id)\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    df=df,\n",
    "    tag2id=tag2id,\n",
    "    feature_dim=in_channels,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "ait_node_data = BGLNodeDataset(root, hparams=hparams)\n",
    "ait_node_loader = DataLoader(ait_node_data, batch_size=4, shuffle=False)\n",
    "b0 = next(iter(ait_node_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "p_a = nn.Parameter(torch.randn(1024))\n",
    "p_b = nn.Parameter(torch.randn(1024))\n",
    "beta = 0.5\n",
    "mu = 0.5\n",
    "gamma = 0.5\n",
    "\n",
    "def score_func(hidden, i, j, weight):\n",
    "    dist = (p_a * hidden[i] + p_b * hidden[j]).norm()**2\n",
    "    print('dist {}, weight {}'.format(dist, weight))\n",
    "    return weight * (beta * (dist - mu)).sigmoid()\n",
    "\n",
    "def neg_sampling(degrees, i, j, s):\n",
    "    # negative sampling\n",
    "    prob_i = degrees[i]/(degrees[i] + degrees[j]) if degrees[i] + degrees[j] else 0\n",
    "    if torch.rand(1) <= prob_i:\n",
    "        # replace node i\n",
    "        i_prime = j\n",
    "        while i_prime == j or s[i_prime, j] != 0:\n",
    "            i_prime = torch.randint(s.size()[0], (1,)).item()\n",
    "        return i_prime, j\n",
    "    else:\n",
    "        # replace node j\n",
    "        j_prime = i\n",
    "        while j_prime == i or s[i, j_prime] != 0:\n",
    "            j_prime = torch.randint(s.size()[0], (1,)).item()\n",
    "        return i, j_prime\n",
    "\n",
    "def margin_loss(hidden, edge_index, s, num_nodes):\n",
    "    degrees = degree(edge_index[0], num_nodes)\n",
    "    scores = []\n",
    "    for i, j in edge_index.T.tolist():\n",
    "        pos_score = score_func(hidden, i, j, s[i, j])\n",
    "        # negative sampling\n",
    "        i_prime, j_prime = neg_sampling(degrees, i, j, s)\n",
    "        neg_score = score_func(hidden, i_prime, j_prime, s[i, j])\n",
    "\n",
    "        if pos_score <= neg_score:\n",
    "            edge_loss = max(0, gamma + pos_score - neg_score)\n",
    "            scores.append(edge_loss)\n",
    "            # edge_loss = F.relu(gamma + pos_score - neg_score)\n",
    "            # print(edge_loss)\n",
    "            # loss += edge_loss\n",
    "            # print(loss)\n",
    "    print(torch.stack(scores))\n",
    "    return sum(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH\n",
    "print(b0, b0.num_graphs)\n",
    "b0.s = to_dense_adj(b0.edge_index)[0]\n",
    "print('feature', b0.x)\n",
    "print('edge index', b0.edge_index)\n",
    "print('adjacency matrix', b0.s)\n",
    "# margin_loss(b0.x, b0.edge_index, b0.s, b0.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph in BATCH\n",
    "for i in range(b0.num_graphs): \n",
    "    print(b0[i])\n",
    "\n",
    "i = 0\n",
    "print('graph #{}: {}'.format(i, b0[i]))\n",
    "print('# of nodes', b0[i].num_nodes)\n",
    "print('feature', b0[i].x)\n",
    "print('edge index', b0[i].edge_index)\n",
    "print('adjacency matrix', to_dense_adj(b0[i].edge_index)[0])\n",
    "print('degree', degree(b0[i].edge_index[0], b0[i].num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree(b0.edge_index[0], b0.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b0.s)\n",
    "print(b0.s.size()[0])\n",
    "print(b0.s[1])\n",
    "print(b0.s[1].nonzero().size()[0])\n",
    "print(b0.s[:,1])\n",
    "print(b0.s[:,1].nonzero().size()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sock Shop Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>@timestamp</th>\n",
       "      <th>container_id</th>\n",
       "      <th>log</th>\n",
       "      <th>container_name</th>\n",
       "      <th>message</th>\n",
       "      <th>ParameterList</th>\n",
       "      <th>NerList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stdout</td>\n",
       "      <td>2023-01-05T19:54:04+00:00</td>\n",
       "      <td>b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...</td>\n",
       "      <td>2023-01-05 19:54:04.558  INFO 7 --- [         ...</td>\n",
       "      <td>/docker-compose-queue-master-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stdout</td>\n",
       "      <td>2023-01-05T19:54:04+00:00</td>\n",
       "      <td>b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...</td>\n",
       "      <td>2023-01-05 19:54:04.604  INFO 7 --- [         ...</td>\n",
       "      <td>/docker-compose-queue-master-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stdout</td>\n",
       "      <td>2023-01-05T19:54:04+00:00</td>\n",
       "      <td>b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...</td>\n",
       "      <td>2023-01-05 19:54:04.634  INFO 7 --- [         ...</td>\n",
       "      <td>/docker-compose-queue-master-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stdout</td>\n",
       "      <td>2023-01-05T19:54:04+00:00</td>\n",
       "      <td>b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...</td>\n",
       "      <td>2023-01-05 19:54:04.787  INFO 7 --- [         ...</td>\n",
       "      <td>/docker-compose-queue-master-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stdout</td>\n",
       "      <td>2023-01-05T19:54:06+00:00</td>\n",
       "      <td>b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...</td>\n",
       "      <td>2023-01-05 19:54:06.603  INFO 7 --- [ost-start...</td>\n",
       "      <td>/docker-compose-queue-master-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                 @timestamp  \\\n",
       "0  stdout  2023-01-05T19:54:04+00:00   \n",
       "1  stdout  2023-01-05T19:54:04+00:00   \n",
       "2  stdout  2023-01-05T19:54:04+00:00   \n",
       "3  stdout  2023-01-05T19:54:04+00:00   \n",
       "4  stdout  2023-01-05T19:54:06+00:00   \n",
       "\n",
       "                                        container_id  \\\n",
       "0  b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...   \n",
       "1  b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...   \n",
       "2  b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...   \n",
       "3  b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...   \n",
       "4  b7a9cccb5f14f1fa7f5c82593112ff5d07523ac0e8b1fe...   \n",
       "\n",
       "                                                 log  \\\n",
       "0  2023-01-05 19:54:04.558  INFO 7 --- [         ...   \n",
       "1  2023-01-05 19:54:04.604  INFO 7 --- [         ...   \n",
       "2  2023-01-05 19:54:04.634  INFO 7 --- [         ...   \n",
       "3  2023-01-05 19:54:04.787  INFO 7 --- [         ...   \n",
       "4  2023-01-05 19:54:06.603  INFO 7 --- [ost-start...   \n",
       "\n",
       "                   container_name message ParameterList NerList  \n",
       "0  /docker-compose-queue-master-1     NaN            []      []  \n",
       "1  /docker-compose-queue-master-1     NaN            []      []  \n",
       "2  /docker-compose-queue-master-1     NaN            []      []  \n",
       "3  /docker-compose-queue-master-1     NaN            []      []  \n",
       "4  /docker-compose-queue-master-1     NaN            []      []  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'dataset/sockshop/database.csv'\n",
    "df = pd.read_csv(file) # 49442 rows\n",
    "df['ParameterList'] = [[] for _ in range(len(df))] # create entity list\n",
    "df['NerList'] = [[] for _ in range(len(df))] # create ner list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    # df['log'].str.contains(\"item\", na = False) &\n",
    "    df['log'].str.contains(\"customer\", na = False)\n",
    "].to_csv('dataset/sockshop/database_customer.csv') # select rows with item & customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 18 2022, 18:57:03) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "63c05d9bc9329713974c38ff46e522d0da350f3f9e20d4e8708b579d2ad1c9e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
